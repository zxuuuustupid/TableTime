import os
import numpy as np
import pandas as pd
from pathlib import Path
from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from scipy import signal
from tqdm import tqdm
import time

def extract_advanced_features(time_series, fs=1000):
    """
    é«˜æ˜çš„ç‰¹å¾æå–ï¼šå¤šå°ºåº¦æ—¶é¢‘ç‰¹å¾èåˆ
    è¾“å…¥: time_series (5000, 6) - å•ä¸ªæ ·æœ¬çš„6é€šé“æ—¶åºæ•°æ®
    è¾“å‡º: feature_vector - èåˆç‰¹å¾å‘é‡
    """
    n_channels = time_series.shape[1]
    all_features = []
    
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        
        # 1. æ—¶åŸŸç‰¹å¾ (12ä¸ª)
        time_features = [
            np.mean(signal_data),
            np.std(signal_data),
            np.max(np.abs(signal_data)),
            np.min(signal_data),
            np.max(signal_data) - np.min(signal_data),  # å³°å³°å€¼
            np.sqrt(np.mean(signal_data**2)),  # RMS
            np.max(np.abs(signal_data)) / np.sqrt(np.mean(signal_data**2)),  # å³°å€¼å› å­
            np.sum(np.abs(np.diff(signal_data))) / (len(signal_data)-1),  # å¹³å‡å˜åŒ–ç‡
            np.mean(np.abs(signal_data - np.mean(signal_data))),  # å¹³å‡ç»å¯¹åå·®
            np.percentile(signal_data, 75) - np.percentile(signal_data, 25),  # å››åˆ†ä½è·
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),  # å³­åº¦
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)  # ååº¦
        ]
        
        # 2. é¢‘åŸŸç‰¹å¾ (10ä¸ª) - ä½¿ç”¨Welchæ–¹æ³•
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=1024, noverlap=512)
        dominant_freq = f[np.argmax(Pxx)]
        spectral_centroid = np.sum(f * Pxx) / np.sum(Pxx)
        spectral_bandwidth = np.sqrt(np.sum(((f - spectral_centroid)**2) * Pxx) / np.sum(Pxx))
        spectral_entropy = -np.sum((Pxx/np.sum(Pxx)) * np.log2(Pxx/np.sum(Pxx) + 1e-10))
        
        freq_features = [
            dominant_freq,
            spectral_centroid,
            spectral_bandwidth,
            spectral_entropy,
            np.max(Pxx),
            np.mean(Pxx),
            np.std(Pxx),
            np.sum(Pxx[:len(f)//4]),  # ä½é¢‘èƒ½é‡
            np.sum(Pxx[len(f)//4:len(f)//2]),  # ä¸­é¢‘èƒ½é‡
            np.sum(Pxx[len(f)//2:])  # é«˜é¢‘èƒ½é‡
        ]
        
        # 3. æ—¶é¢‘åŸŸç‰¹å¾ (4ä¸ª) - é¢‘å¸¦ç»Ÿè®¡ç‰¹å¾
        f, Pxx = signal.welch(signal_data, fs=1000, nperseg=1024)
        total_power = np.sum(Pxx)
        low_freq = np.sum(Pxx[f <= 50]) / total_power if total_power > 0 else 0
        mid_freq = np.sum(Pxx[(f > 50) & (f <= 200)]) / total_power if total_power > 0 else 0
        high_freq = np.sum(Pxx[f > 200]) / total_power if total_power > 0 else 0
        spectral_flatness = np.exp(np.mean(np.log(Pxx + 1e-10))) / np.mean(Pxx + 1e-10)
        wavelet_features = [low_freq, mid_freq, high_freq, spectral_flatness]
        
        # åˆå¹¶æ‰€æœ‰ç‰¹å¾
        channel_features = np.concatenate([time_features, freq_features, wavelet_features])
        all_features.append(channel_features)
    
    # 4. é€šé“é—´ç›¸å…³æ€§ç‰¹å¾ (15ä¸ª)
    correlation_features = []
    for i in range(n_channels):
        for j in range(i+1, n_channels):
            corr = np.corrcoef(time_series[:, i], time_series[:, j])[0, 1]
            correlation_features.append(corr)
    
    # 5. å¤šå°ºåº¦ç»Ÿè®¡ç‰¹å¾ (6ä¸ª)
    multi_scale_features = []
    signal_data = time_series[:, 0]  # ä½¿ç”¨ç¬¬ä¸€ä¸ªé€šé“è¿›è¡Œå¤šå°ºåº¦åˆ†æ
    for scale in [100, 500, 1000, 2500]:
        if len(signal_data) > scale:
            segments = np.array_split(signal_data, len(signal_data)//scale)
            segment_means = [np.mean(seg) for seg in segments]
            segment_stds = [np.std(seg) for seg in segments]
            multi_scale_features.extend([
                np.std(segment_means),
                np.std(segment_stds),
                np.max(segment_means) - np.min(segment_means),
                np.max(segment_stds) - np.min(segment_stds),
                np.mean(np.abs(np.diff(segment_means))),
                np.mean(np.abs(np.diff(segment_stds)))
            ])
    
    # æœ€ç»ˆç‰¹å¾å‘é‡
    final_features = np.concatenate([
        np.array(all_features).flatten(),
        np.array(correlation_features),
        np.array(multi_scale_features)
    ])
    
    return final_features

def adaptive_distance_metric(X_train, y_train, n_neighbors=10):
    """
    é«˜æ˜çš„è‡ªé€‚åº”è·ç¦»åº¦é‡å­¦ä¹ 
    é€šè¿‡åˆ†æè®­ç»ƒæ•°æ®çš„åˆ†å¸ƒç‰¹æ€§ï¼Œè‡ªé€‚åº”è°ƒæ•´ä¸åŒç‰¹å¾ç»´åº¦çš„æƒé‡
    """
    from sklearn.preprocessing import MinMaxScaler
    
    # 1. è®¡ç®—æ¯ä¸ªæ ·æœ¬çš„å±€éƒ¨å¯†åº¦
    nbrs = NearestNeighbors(n_neighbors=n_neighbors, algorithm='auto').fit(X_train)
    distances, indices = nbrs.kneighbors(X_train)
    local_density = 1.0 / (np.mean(distances[:, 1:], axis=1) + 1e-10)
    
    # 2. ä¸ºæ¯ä¸ªç±»åˆ«è®¡ç®—ç‰¹å¾é‡è¦æ€§
    unique_classes = np.unique(y_train)
    class_weights = np.zeros((len(unique_classes), X_train.shape[1]))
    
    for i, cls in enumerate(unique_classes):
        class_mask = (y_train == cls)
        class_data = X_train[class_mask]
        
        if len(class_data) > 1:
            # è®¡ç®—ç±»å†…æ–¹å·®
            class_var = np.var(class_data, axis=0)
            # ç±»å†…æ–¹å·®å°çš„ç‰¹å¾æ›´é‡è¦ï¼ˆæ›´å…·æœ‰åˆ¤åˆ«æ€§ï¼‰
            feature_importance = 1.0 / (class_var + np.mean(class_var))
            class_weights[i] = feature_importance / np.sum(feature_importance)
    
    # 3. ä¸ºæ¯ä¸ªæ ·æœ¬åˆ†é…æƒé‡ï¼ˆåŸºäºå±€éƒ¨å¯†åº¦å’Œç±»åˆ«ï¼‰
    sample_weights = np.zeros((X_train.shape[0], X_train.shape[1]))
    for i in range(X_train.shape[0]):
        cls = y_train[i]
        cls_idx = np.where(unique_classes == cls)[0][0]
        # ç»“åˆå±€éƒ¨å¯†åº¦å’Œç±»åˆ«æƒé‡
        sample_weights[i] = class_weights[cls_idx] * (local_density[i] / np.max(local_density))
    
    # 4. å½’ä¸€åŒ–æƒé‡
    scaler = MinMaxScaler()
    sample_weights = scaler.fit_transform(sample_weights)
    
    return sample_weights

def sophisticated_clustering_analysis(target_condition, source_conditions, fault_types):
    """
    é«˜æ˜çš„èšç±»åˆ†æç¨‹åº
    å‚æ•°:
    - target_condition: int, ç›®æ ‡å·¥å†µï¼ˆæ•°æ®Aï¼‰
    - source_conditions: list of int, æºå·¥å†µåˆ—è¡¨ï¼ˆæ•°æ®Bï¼‰
    - fault_types: list of int, è¦èšç±»çš„æ•…éšœç±»å‹åˆ—è¡¨
    """
    
    # =============== 1. è·¯å¾„è®¾ç½® ===============
    current_dir = Path(__file__).parent.absolute() if '__file__' in globals() else Path.cwd()
    output_dir = current_dir / "output"
    result_dir = current_dir / "result"
    result_dir.mkdir(parents=True, exist_ok=True)
    
    # =============== 2. åŠ è½½æ•°æ® ===============
    # åŠ è½½ç›®æ ‡æ•°æ®A
    data_A = []
    labels_A = []
    
    for fault_type in fault_types:
        file_path = output_dir / f"G{fault_type}_WC{target_condition}.npy"
        if not file_path.exists():
            continue
        
        samples = np.load(file_path)[:50]
        data_A.append(samples)
        labels_A.extend([fault_type] * len(samples))
    
    if not data_A:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç›®æ ‡æ•°æ®A")
    
    data_A = np.vstack(data_A)
    labels_A = np.array(labels_A)
    
    # åŠ è½½æºæ•°æ®B
    data_B = []
    labels_B = []
    
    for condition in source_conditions:
        if condition == target_condition:  # ç¡®ä¿æ’é™¤ç›®æ ‡å·¥å†µ
            continue
        for fault_type in fault_types:
            file_path = output_dir / f"G{fault_type}_WC{condition}.npy"
            if not file_path.exists():
                continue
            samples = np.load(file_path)[:50]
            data_B.append(samples)
            labels_B.extend([fault_type] * len(samples))
    
    if not data_B:
        raise ValueError("æ²¡æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„æºæ•°æ®B")
    
    data_B = np.vstack(data_B)
    labels_B = np.array(labels_B)
    
    # =============== 3. é«˜çº§ç‰¹å¾æå– ===============
    # ä¸ºAæå–ç‰¹å¾
    features_A = []
    for i in range(len(data_A)):
        features = extract_advanced_features(data_A[i])
        features_A.append(features)
    features_A = np.array(features_A)
    
    # ä¸ºBæå–ç‰¹å¾
    features_B = []
    for i in range(len(data_B)):
        features = extract_advanced_features(data_B[i])
        features_B.append(features)
    features_B = np.array(features_B)
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    features_B_scaled = scaler.fit_transform(features_B)
    features_A_scaled = scaler.transform(features_A)
    
    # =============== 4. è‡ªé€‚åº”è·ç¦»åº¦é‡å­¦ä¹  ===============
    adaptive_weights = adaptive_distance_metric(features_B_scaled, labels_B)
    
    # åº”ç”¨è‡ªé€‚åº”æƒé‡
    weighted_features_B = features_B_scaled * adaptive_weights.mean(axis=0)
    weighted_features_A = features_A_scaled * adaptive_weights.mean(axis=0)
    
    # =============== 5. é‚»å±…æœç´¢ ===============
    nbrs = NearestNeighbors(n_neighbors=50, algorithm='auto', metric='euclidean', n_jobs=-1)
    nbrs.fit(weighted_features_B)
    distances, indices = nbrs.kneighbors(weighted_features_A)
    
    # =============== 6. ä¿å­˜ç»“æœ ===============
    result_file = result_dir / f"clustering_result_C{target_condition}_vs_{'_'.join(map(str, source_conditions))}_faults{'_'.join(map(str, fault_types))}.npz"
    np.savez(result_file,
             distances=distances,
             neighbor_indices=indices,
             labels_A=labels_A,
             labels_B=labels_B)
    
    # =============== 7. èšç±»å‡†ç¡®ç‡è®¡ç®—ï¼ˆä¿®æ­£ä¸ºé‚»å±…çº¯åº¦ï¼‰==============
    # è·å–æ¯ä¸ªAæ ·æœ¬å¯¹åº”çš„50ä¸ªBé‚»å±…çš„çœŸå®æ ‡ç­¾
    neighbor_labels = labels_B[indices]  # shape: (N_A, 50)
    
    # è®¡ç®—æ¯ä¸ªAæ ·æœ¬çš„50ä¸ªé‚»å±…ä¸­ï¼Œä¸è‡ªèº«çœŸå®æ ‡ç­¾ä¸€è‡´çš„æ¯”ä¾‹
    match_ratios = np.mean(neighbor_labels == labels_A[:, None], axis=1)
    
    # æ€»ä½“çº¯åº¦ï¼šæ‰€æœ‰Aæ ·æœ¬çš„å¹³å‡ä¸€è‡´æ¯”ä¾‹
    total_accuracy = np.mean(match_ratios)
    
    # æŒ‰æ•…éšœç±»å‹è®¡ç®—çº¯åº¦
    class_accuracies = {}
    for fault_type in fault_types:
        mask = (labels_A == fault_type)
        if np.sum(mask) > 0:
            class_acc = np.mean(match_ratios[mask])
            class_accuracies[fault_type] = class_acc
    
    return {
        'total_accuracy': total_accuracy,
        'class_accuracies': class_accuracies,
        'result_file': result_file
    }


if __name__ == "__main__":
    import argparse

    # =============== è¾“å…¥æ¥å£é…ç½® ===============
    parser = argparse.ArgumentParser(description="è·¨å·¥å†µæœºæ¢°æ•…éšœèšç±»åˆ†æå·¥å…·")
    
    # æ¥å£ 1: é€‰æ‹©å•ä¸€ç›®æ ‡å·¥å†µ (å¾…æµ‹æ•°æ®)
    parser.add_argument('--target', type=int, default=1, 
                        help='æŒ‡å®šå¾…æµ‹çš„ç›®æ ‡å·¥å†µç¼–å· (ä¾‹å¦‚: 1)')
    
    # æ¥å£ 2: é€‰æ‹©å¤šä¸ªæºå·¥å†µ (ç”¨äºæ£€ç´¢çš„æ ·æœ¬æ± )
    parser.add_argument('--sources', type=int, nargs='+', default=[2, 3, 4], 
                        help='æŒ‡å®šåŒ…å«åœ¨æ£€ç´¢æ ·æœ¬æ± ä¸­çš„å·¥å†µç¼–å·åˆ—è¡¨ (ä¾‹å¦‚: 2 3 4)')
    
    # æ¥å£ 3: é€‰æ‹©æ•…éšœç±»å‹ (å¯é€‰)
    parser.add_argument('--faults', type=int, nargs='+', default=list(range(0, 9)), 
                        help='æŒ‡å®šå‚ä¸åˆ†æçš„æ•…éšœç±»å‹ (é»˜è®¤ 0-8)')

    args = parser.parse_args()

    # å°†è¾“å…¥èµ‹å€¼ç»™å˜é‡
    target_wc = args.target
    source_wcs = args.sources
    fault_list = args.faults

    # =============== æ‰§è¡Œæ ¡éªŒä¸åˆ†æ ===============
    print("="*40)
    print(f"ğŸš€ å¼€å§‹å®éªŒåˆ†æ")
    print(f"ğŸ“ å¾…æµ‹ç›®æ ‡å·¥å†µ (Target): WC{target_wc}")
    print(f"ğŸ“š æ£€ç´¢æ ·æœ¬æ¥æº (Sources): {[f'WC{c}' for c in source_wcs]}")
    print(f"ğŸ› ï¸ æ•…éšœç±»å‹èŒƒå›´: G{fault_list[0]} - G{fault_list[-1]}")
    print("="*40)

    # é€»è¾‘æ£€æŸ¥ï¼šé˜²æ­¢ç›®æ ‡å·¥å†µå‡ºç°åœ¨æºå·¥å†µä¸­ï¼ˆå¯¼è‡´æ•°æ®æ³„éœ²ï¼‰
    if target_wc in source_wcs:
        print(f"âš ï¸ è­¦å‘Š: ç›®æ ‡å·¥å†µ {target_wc} åŒæ—¶ä¹Ÿå‡ºç°åœ¨æºå·¥×§×•×ªåˆ—è¡¨ä¸­ï¼")
        print(f"ç³»ç»Ÿå°†è‡ªåŠ¨ä»æ£€ç´¢æ± ä¸­å‰”é™¤å·¥å†µ {target_wc} ä»¥ä¿è¯å®éªŒä¸¥è°¨æ€§ã€‚")
        source_wcs = [c for c in source_wcs if c != target_wc]

    try:
        # æ‰§è¡Œæ ¸å¿ƒåˆ†æé€»è¾‘
        result = sophisticated_clustering_analysis(
            target_condition=target_wc,
            source_conditions=source_wcs,
            fault_types=fault_list
        )

        # =============== è¾“å‡ºæœ¬æ¬¡ç‰¹å®šå®éªŒçš„ç»“æœ ===============
        print("\n" + "âœ… åˆ†æå®Œæˆ".center(34, "-"))
        print(f"æ€»ä½“å‡†ç¡®ç‡ (Retrieval Purity): {result['total_accuracy']:.4f}")
        print("-" * 40)
        print("å„æ•…éšœç±»å‹å‡†ç¡®ç‡ç»†èŠ‚:")
        for f_type, acc in result['class_accuracies'].items():
            print(f"  æ•…éšœ G{f_type}: {acc:.4f}")
        print("-" * 40)
        print(f"ç»“æœæ–‡ä»¶å·²ä¿å­˜è‡³: {result['result_file']}")

    except Exception as e:
        print(f"âŒ è¿è¡Œå¤±è´¥: {str(e)}")

# if __name__ == "__main__":
#     # æ‰€æœ‰å·¥å†µåˆ—è¡¨
#     all_conditions = list(range(1, 10))
#     # æ‰€æœ‰æ•…éšœç±»å‹
#     all_fault_types = list(range(0, 9))
    
#     # å­˜å‚¨æ‰€æœ‰ç»“æœ
#     all_results = {}
    
#     # å¾ªç¯å¤„ç†æ¯ç§å·¥å†µä½œä¸ºç›®æ ‡
#     for target_condition in all_conditions:
#         # æºå·¥å†µ = æ‰€æœ‰å·¥å†µ - ç›®æ ‡å·¥å†µ
#         source_conditions = [c for c in all_conditions if c != target_condition]
        
#         try:
#             # æ‰§è¡Œèšç±»åˆ†æ
#             result = sophisticated_clustering_analysis(
#                 target_condition=target_condition,
#                 source_conditions=source_conditions,
#                 fault_types=all_fault_types
#             )
            
#             # ä¿å­˜ç»“æœ
#             all_results[target_condition] = result
            
#         except Exception as e:
#             print(f"âŒ å·¥å†µ {target_condition} å¤„ç†å¤±è´¥: {str(e)}")
#             continue
    
#     # è¾“å‡ºæœ€ç»ˆç»“æœ
#     print("å·¥å†µ\tæ€»ä½“å‡†ç¡®ç‡")
#     print("-" * 20)
#     total_accuracies = []
#     for condition in sorted(all_results.keys()):
#         acc = all_results[condition]['total_accuracy']
#         total_accuracies.append(acc)
#         print(f"{condition}\t{acc:.4f}")
    
#     if total_accuracies:
#         avg_accuracy = np.mean(total_accuracies)
#         max_accuracy = np.max(total_accuracies)
#         min_accuracy = np.min(total_accuracies)
#         max_cond = sorted(all_results.keys())[np.argmax(total_accuracies)]
#         min_cond = sorted(all_results.keys())[np.argmin(total_accuracies)]
        
#         print("-" * 20)
#         print(f"å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.4f}")
#         print(f"æœ€é«˜å‡†ç¡®ç‡: {max_accuracy:.4f} (å·¥å†µ {max_cond})")
#         print(f"æœ€ä½å‡†ç¡®ç‡: {min_accuracy:.4f} (å·¥å†µ {min_cond})")