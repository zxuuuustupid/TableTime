# import numpy as np
# import os
# from tqdm import tqdm
# from sklearn.preprocessing import StandardScaler
# from sklearn.neighbors import NearestNeighbors
# from scipy import signal

# # --- 1. é…ç½®è·¯å¾„ä¸å‚æ•° ---
# DATASET = "BJTU-gearbox"
# BASE_PATH = f"few_shot_test/data/{DATASET}"
# NUM_NEIGHBORS = 1

# # --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° (å®Œå…¨ä¿ç•™ä½ çš„æ•°å­¦é€»è¾‘) ---

# def load_wc_data(wc_id):
#     path = os.path.join(BASE_PATH, f"WC{wc_id}")
#     # å¼ºåˆ¶è½¬ä¸º float32 é˜²æ­¢ç²¾åº¦å¼•èµ·çš„å¾®å°å·®å¼‚
#     x_train = np.load(os.path.join(path, 'X_train.npy'), mmap_mode='c').astype(np.float32)
#     y_train = np.load(os.path.join(path, 'y_train.npy'), mmap_mode='c').astype(str)
#     x_valid = np.load(os.path.join(path, 'X_valid.npy'), mmap_mode='c').astype(np.float32)
#     y_valid = np.load(os.path.join(path, 'y_valid.npy'), mmap_mode='c').astype(str)
#     return x_train, y_train, x_valid, y_valid

# def extract_advanced_features(time_series, fs=1000):
#     if time_series.shape[0] < time_series.shape[1]: 
#         time_series = time_series.T
#     n_channels = time_series.shape[1]
#     all_features = []
#     for ch in range(n_channels):
#         signal_data = time_series[:, ch]
#         # æ—¶åŸŸç‰¹å¾ 8ä¸ª
#         time_features = [
#             np.mean(signal_data), np.std(signal_data), np.max(np.abs(signal_data)),
#             np.max(signal_data) - np.min(signal_data), np.sqrt(np.mean(signal_data**2)),
#             np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
#             np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
#             np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
#         ]
#         # é¢‘åŸŸç‰¹å¾ 5ä¸ª
#         f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
#         freq_features = [f[np.argmax(Pxx)], np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10), np.max(Pxx), np.mean(Pxx), np.std(Pxx)]
#         all_features.extend(time_features + freq_features)
#     # é€šé“ç›¸å…³æ€§
#     correlation_features = [np.corrcoef(time_series[:, i], time_series[:, j])[0, 1] 
#                             for i in range(n_channels) for j in range(i+1, n_channels)]
#     return np.concatenate([np.array(all_features), np.nan_to_num(np.array(correlation_features))])

# def fiw_core_logic(search_feat, search_label, test_feat, test_label, num_neighbors):
#     """
#     è¿™æ˜¯ä½ çš„ FIW æ ¸å¿ƒç®—æ³•é€»è¾‘å®ç°
#     """
#     # 1. è¿™é‡Œçš„æ ‡å‡†åŒ–åŸºå‡†éå¸¸é‡è¦ï¼Œå¿…é¡»ç¡®ä¿ search_feat åŒ…å«äº†æ‰€æœ‰å‚è€ƒä¿¡æ¯
#     scaler = StandardScaler()
#     search_scaled = scaler.fit_transform(search_feat)
#     test_scaled = scaler.transform(test_feat)

#     # 2. æœ‰ç›‘ç£ç‰¹å¾æƒé‡è®¡ç®—
#     unique_classes = np.unique(search_label)
#     n_features = search_scaled.shape[1]
#     feature_weights = np.zeros(n_features)
    
#     for label in unique_classes:
#         class_data = search_scaled[search_label == label]
#         if len(class_data) > 1:
#             class_var = np.var(class_data, axis=0)
#             weight = 1.0 / (class_var + 1e-5)
#             feature_weights += weight
    
#     feature_weights = feature_weights / len(unique_classes)
#     feature_weights = feature_weights / (np.max(feature_weights) + 1e-10)

#     # 3. åº”ç”¨æƒé‡
#     search_weighted = search_scaled * feature_weights
#     test_weighted = test_scaled * feature_weights

#     # 4. æ£€ç´¢
#     nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='euclidean', n_jobs=-1)
#     nbrs.fit(search_weighted)
#     _, indices = nbrs.kneighbors(test_weighted)

#     # 5. ç»Ÿè®¡ç»†èŠ‚
#     all_labels = np.unique(test_label)
#     label_purities = {}
#     total_correct = 0
    
#     for lbl in all_labels:
#         idx_in_test = np.where(test_label == lbl)[0]
#         lbl_correct = 0
#         for i in idx_in_test:
#             hits = np.sum(search_label[indices[i]] == lbl)
#             lbl_correct += hits
#             total_correct += hits
#         label_purities[lbl] = (lbl_correct / (len(idx_in_test) * num_neighbors)) * 100

#     overall_purity = (total_correct / (len(test_label) * num_neighbors)) * 100
#     return overall_purity, label_purities

# # --- 3. å®éªŒæµç¨‹æ§åˆ¶ ---

# def run_custom_experiment(target_wc, source_wcs):
#     print(f"\nğŸš€ å¯åŠ¨å®éªŒ: Target=WC{target_wc} | Sources={source_wcs}")

#     # A. åŠ è½½ç›®æ ‡å·¥å†µ
#     xt_train, yt_train, xt_valid, yt_valid = load_wc_data(target_wc)
#     feat_t_train = np.array([extract_advanced_features(s) for s in tqdm(xt_train, desc="Target Train")])
#     feat_t_valid = np.array([extract_advanced_features(s) for s in tqdm(xt_valid, desc="Target Valid")])

#     # B. åŠ è½½æºå·¥å†µ
#     all_s_feats, all_s_labels = [], []
#     for s_id in source_wcs:
#         xs_train, ys_train, _, _ = load_wc_data(s_id)
#         s_feat = np.array([extract_advanced_features(s) for s in tqdm(xs_train, desc=f"WC{s_id} Train")])
#         all_s_feats.append(s_feat)
#         all_s_labels.append(ys_train)

#     scope1_feat = np.concatenate(all_s_feats, axis=0)
#     scope1_label = np.concatenate(all_s_labels, axis=0)

#     # æ–¹æ¡ˆ 2 çš„åº“
#     scope2_feat = np.concatenate([scope1_feat, feat_t_train], axis=0)
#     scope2_label = np.concatenate([scope1_label, yt_train], axis=0)

#     # C. è®¡ç®—ç»“æœ
#     # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯åŒä¸€ä¸ªæ ¸å¿ƒé€»è¾‘å‡½æ•°
#     overall1, detailed1 = fiw_core_logic(scope1_feat, scope1_label, feat_t_valid, yt_valid, NUM_NEIGHBORS)
#     overall2, detailed2 = fiw_core_logic(scope2_feat, scope2_label, feat_t_valid, yt_valid, NUM_NEIGHBORS)

#     # D. æ‰“å°æŠ¥å‘Š
#     print(f"\n{'æ•…éšœæ ‡ç­¾':<10} | {'æ–¹æ¡ˆ1(S)':<12} | {'æ–¹æ¡ˆ2(S+T)':<12} | {'æå‡'}")
#     print("-" * 50)
#     for lbl in sorted(detailed1.keys()):
#         p1, p2 = detailed1[lbl], detailed2[lbl]
#         print(f"{lbl:<14} | {p1:>11.2f}% | {p2:>11.2f}% | {p2-p1:>+7.2f}%")
#     print("-" * 50)
#     print(f"{'æ€»ä½“å¹³å‡':<14} | {overall1:>11.2f}% | {overall2:>11.2f}% | {overall2-overall1:>+7.2f}%")

# if __name__ == "__main__":
#     run_custom_experiment(target_wc=8, source_wcs=[1, 2, ])


import numpy as np
import os
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from scipy import signal

# --- 1. é…ç½®è·¯å¾„ä¸å‚æ•° ---
DATASET = "BJTU-gearbox"
BASE_PATH = f"few_shot_test/data/{DATASET}"
NUM_NEIGHBORS = 3  # è®¾å®šä¸º1ï¼Œåªæ‰¾æœ€è¿‘çš„ä¸€ä¸ª

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def load_wc_data(wc_id):
    path = os.path.join(BASE_PATH, f"WC{wc_id}")
    x_train = np.load(os.path.join(path, 'X_train.npy'), mmap_mode='c').astype(np.float32)
    y_train = np.load(os.path.join(path, 'y_train.npy'), mmap_mode='c').astype(str)
    x_valid = np.load(os.path.join(path, 'X_valid.npy'), mmap_mode='c').astype(np.float32)
    y_valid = np.load(os.path.join(path, 'y_valid.npy'), mmap_mode='c').astype(str)
    return x_train, y_train, x_valid, y_valid

def extract_advanced_features(time_series, fs=1000):
    """æå–é«˜çº§ç‰¹å¾ (ä¿æŒä¸å˜)"""
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
    n_channels = time_series.shape[1]
    all_features = []
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        time_features = [
            np.mean(signal_data), np.std(signal_data), np.max(np.abs(signal_data)),
            np.max(signal_data) - np.min(signal_data), np.sqrt(np.mean(signal_data**2)),
            np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
        ]
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
        freq_features = [f[np.argmax(Pxx)], np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10), np.max(Pxx), np.mean(Pxx), np.std(Pxx)]
        all_features.extend(time_features + freq_features)
    correlation_features = [np.corrcoef(time_series[:, i], time_series[:, j])[0, 1] 
                            for i in range(n_channels) for j in range(i+1, n_channels)]
    return np.concatenate([np.array(all_features), np.nan_to_num(np.array(correlation_features))])

def euclidean_core_logic(search_feat, search_label, test_feat, test_label, num_neighbors):
    """
    [æ ¸å¿ƒæ”¹åŠ¨]ï¼šæ— ç›‘ç£é«˜çº§æ¬§æ°è·ç¦»æ£€ç´¢é€»è¾‘
    """
    # 1. æ•´ä½“æ ‡å‡†åŒ– (é«˜çº§æ¬§æ°è·ç¦»çš„åŸºç¡€)
    scaler = StandardScaler()
    search_scaled = scaler.fit_transform(search_feat)
    test_scaled = scaler.transform(test_feat)

    # 2. ç›´æ¥è¿›è¡Œæœ€è¿‘é‚»æœç´¢ (ä¸å†è®¡ç®— feature_weights)
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='euclidean', n_jobs=-1)
    nbrs.fit(search_scaled)
    _, indices = nbrs.kneighbors(test_scaled)

    # 3. ç»Ÿè®¡ç»“æœ (ä»…ç”¨äºè¯„ä¼°ï¼Œæ£€ç´¢è¿‡ç¨‹ä¸ä½¿ç”¨æ ‡ç­¾)
    all_labels = np.unique(test_label)
    label_purities = {}
    total_correct = 0
    
    for lbl in all_labels:
        idx_in_test = np.where(test_label == lbl)[0]
        lbl_correct = 0
        for i in idx_in_test:
            # æ£€æŸ¥æœ€è¿‘é‚»çš„æ ‡ç­¾æ˜¯å¦ä¸çœŸå€¼ä¸€è‡´
            hits = np.sum(search_label[indices[i]] == lbl)
            lbl_correct += hits
            total_correct += hits
        label_purities[lbl] = (lbl_correct / (len(idx_in_test) * num_neighbors)) * 100

    overall_purity = (total_correct / (len(test_label) * num_neighbors)) * 100
    return overall_purity, label_purities

# --- 3. å®éªŒæµç¨‹æ§åˆ¶ ---

def run_custom_experiment(target_wc, source_wcs):
    print(f"\nğŸš€ å¯åŠ¨[æ— ç›‘ç£æ¬§æ°è·ç¦»]å®éªŒ: Target=WC{target_wc} | Sources={source_wcs}")

    xt_train, yt_train, xt_valid, yt_valid = load_wc_data(target_wc)
    feat_t_train = np.array([extract_advanced_features(s) for s in tqdm(xt_train, desc="Target Train")])
    feat_t_valid = np.array([extract_advanced_features(s) for s in tqdm(xt_valid, desc="Target Valid")])

    all_s_feats, all_s_labels = [], []
    for s_id in source_wcs:
        xs_train, ys_train, _, _ = load_wc_data(s_id)
        s_feat = np.array([extract_advanced_features(s) for s in tqdm(xs_train, desc=f"WC{s_id} Train")])
        all_s_feats.append(s_feat)
        all_s_labels.append(ys_train)

    scope1_feat = np.concatenate(all_s_feats, axis=0)
    scope1_label = np.concatenate(all_s_labels, axis=0)

    scope2_feat = np.concatenate([scope1_feat, feat_t_train], axis=0)
    scope2_label = np.concatenate([scope1_label, yt_train], axis=0)

    # æ‰§è¡Œæ£€ç´¢
    overall1, detailed1 = euclidean_core_logic(scope1_feat, scope1_label, feat_t_valid, yt_valid, NUM_NEIGHBORS)
    overall2, detailed2 = euclidean_core_logic(scope2_feat, scope2_label, feat_t_valid, yt_valid, NUM_NEIGHBORS)

    print(f"\n{'æ•…éšœæ ‡ç­¾':<10} | {'æ–¹æ¡ˆ1(S)':<12} | {'æ–¹æ¡ˆ2(S+T)':<12} | {'æå‡'}")
    print("-" * 50)
    for lbl in sorted(detailed1.keys()):
        p1, p2 = detailed1[lbl], detailed2[lbl]
        print(f"{lbl:<14} | {p1:>11.2f}% | {p2:>11.2f}% | {p2-p1:>+7.2f}%")
    print("-" * 50)
    print(f"{'æ€»ä½“å¹³å‡':<14} | {overall1:>11.2f}% | {overall2:>11.2f}% | {overall2-overall1:>+7.2f}%")

if __name__ == "__main__":
    run_custom_experiment(target_wc=6, source_wcs=[1, 2])