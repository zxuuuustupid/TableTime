import json
import sys
import os

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€å±‚ç›®å½•ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if root_path not in sys.path:
    sys.path.insert(0, root_path)

import os
import numpy as np
import json
from dtaidistance import dtw_ndim
from tqdm import tqdm
from scipy.spatial.distance import cdist
from scipy.stats import skew, kurtosis
from scipy.fft import fft, fftfreq
import numpy as np
import os
from tqdm import tqdm
from scipy.spatial.distance import cdist
from scipy.stats import skew, kurtosis
from scipy.fft import fft
from sklearn.decomposition import PCA

# def standardize(X):
#     means = np.mean(X, axis=1, keepdims=True)
#     stds = np.std(X, axis=1, keepdims=True)
#     Z = (X - means) / stds
#     return Z

# def standard_ED(X,Y):
#     X_standard=standardize(X)
#     Y_standard=standardize(Y)
#     return np.linalg.norm(Y_standard-X_standard)

# # def find_nearest_neighbors_DTW(train_data, test_data, num_neighbors=2):
# #     results = []
# #     for test_index, test_seq in tqdm(enumerate(test_data)):
# #         distances = [dtw_ndim.distance(test_seq, train_seq) for train_seq in train_data]
# #         nearest_indices = np.argsort(distances)[:num_neighbors]
# #         results.append({"test_index": test_index, "neighbors": nearest_indices.tolist()})
# #     return results

# def find_nearest_neighbors_DTW(train_data, test_data, num_neighbors=2):
#     results = []
#     for test_index, test_seq in tqdm(enumerate(test_data), desc="DTW Normalized"):
        
#         # [å…³é”®] å…ˆå¯¹æµ‹è¯•åºåˆ—è¿›è¡Œæ ‡å‡†åŒ–
#         test_seq_std = standardize(test_seq)
        
#         # åœ¨è®¡ç®—è·ç¦»æ—¶ï¼Œå¯¹æ¯ä¸€ä¸ªè®­ç»ƒåºåˆ—ä¹Ÿè¿›è¡Œæ ‡å‡†åŒ–
#         distances = [dtw_ndim.distance(test_seq_std, standardize(train_seq)) for train_seq in train_data]
        
#         nearest_indices = np.argsort(distances)[:num_neighbors]
#         results.append({"test_index": test_index, "neighbors": nearest_indices.tolist()})
#     return results


# def find_nearest_neighbors_ED(train_data,test_data,num_neighbors):
#     results=[]
#     for test_index,test_seq in tqdm(enumerate(test_data)):
#         distances = [np.linalg.norm(test_seq-train_seq) for train_seq in train_data]
#         nearest_indices = np.argsort(distances)[:num_neighbors]
#         results.append({"test_index": test_index, "neighbors": nearest_indices.tolist()})
#     return results

# def find_nearest_neighbors_standard_ED(train_data,test_data,num_neighbors):
#     results=[]
#     for test_index,test_seq in tqdm(enumerate(test_data)):
#         distances = [standard_ED(test_seq,train_seq) for train_seq in train_data]
#         nearest_indices = np.argsort(distances)[:num_neighbors]
#         results.append({"test_index": test_index, "neighbors": nearest_indices.tolist()})
#     return results

# def find_nearest_neighbors_MAN(train_data,test_data,num_neighbors):
#     result=[]
#     for test_index,test_seq in tqdm(enumerate(test_data)):
#         distances = [np.sum(np.abs(test_seq-train_seq)) for train_seq in train_data]
#         nearest_indices = np.argsort(distances)[:num_neighbors]
#         result.append({"test_index": test_index, "neighbors": nearest_indices.tolist()})
#     return result

# def calculate_feature_vector(sample_data, fs=64000):
#     """
#     ä¸ºå•ä¸ªæ ·æœ¬ (Channels x TimePoints) è®¡ç®—ä¸€ä¸ªæ‰å¹³åŒ–çš„ç‰¹å¾å‘é‡ã€‚
#     """
#     num_channels, num_points = sample_data.shape
#     all_channel_features = []

#     for i in range(num_channels):
#         signal = sample_data[i]
        
#         # æ—¶åŸŸç‰¹å¾
#         rms = np.sqrt(np.mean(signal**2))
#         peak = np.max(np.abs(signal))
#         crest_factor = peak / rms if rms > 0 else 0
#         kur = kurtosis(signal, fisher=False)
#         skw = skew(signal)
        
#         # é¢‘åŸŸç‰¹å¾
#         fft_vals = np.abs(fft(signal))[:num_points//2]
#         freqs = fftfreq(num_points, 1/fs)[:num_points//2]
#         dominant_freq = freqs[np.argmax(fft_vals)] if len(fft_vals) > 0 else 0
#         spectral_centroid = np.sum(freqs * fft_vals) / np.sum(fft_vals) if np.sum(fft_vals) > 0 else 0

#         channel_features = [rms, peak, crest_factor, kur, skw, dominant_freq, spectral_centroid]
#         all_channel_features.extend(channel_features)
        
#     return np.array(all_channel_features)

# def calculate_robust_feature_vector(sample_data, fs=64000):
#     """
#     æ”¹è¿›ç‰ˆç‰¹å¾æå–ï¼šä¸“æ³¨äºæ— é‡çº²æŒ‡æ ‡ï¼Œå‡å°‘å—è½¬é€Ÿå½±å“çš„èƒ½é‡æŒ‡æ ‡ã€‚
#     """
#     num_channels, num_points = sample_data.shape
#     all_channel_features = []

#     for i in range(num_channels):
#         signal = sample_data[i]
        
#         # 1. åŸºç¡€ç»Ÿè®¡é‡
#         rms = np.sqrt(np.mean(signal**2)) + 1e-9
#         peak = np.max(np.abs(signal))
#         abs_mean = np.mean(np.abs(signal)) + 1e-9
        
#         # 2. æ— é‡çº²æŒ‡æ ‡ (è¿™äº›æŒ‡æ ‡å¯¹è½¬é€Ÿä¸æ•æ„Ÿï¼Œåªå¯¹ä¿¡å·å½¢çŠ¶æ•æ„Ÿ)
#         kur = kurtosis(signal, fisher=False)  # å³­åº¦ï¼šåæ˜ å†²å‡»æ€§
#         skw = skew(signal)                     # ååº¦ï¼šåæ˜ åˆ†å¸ƒå¯¹ç§°æ€§
#         crest = peak / rms                     # å³°å€¼å› å­
#         shape = rms / abs_mean                 # æ³¢å½¢å› å­
#         impulse = peak / abs_mean              # è„‰å†²å› å­
        
#         # 3. é¢‘åŸŸå½’ä¸€åŒ–ç‰¹å¾
#         fft_vals = np.abs(fft(signal))[:num_points//2]
#         # ä½¿ç”¨èƒ½é‡å½’ä¸€åŒ–é¢‘è°±ï¼Œå…³æ³¨é¢‘è°±å½¢çŠ¶è€Œéç»å¯¹å¼ºåº¦
#         norm_fft = fft_vals / (np.sum(fft_vals) + 1e-9)
        
#         # æå–é¢‘åŸŸå‰3ä¸ªä¸»å³°çš„ç›¸å¯¹èƒ½é‡åˆ†å¸ƒï¼ˆä»£æ›¿ç»å¯¹é¢‘ç‡ä½ç½®ï¼‰
#         top_peaks = np.sort(norm_fft)[-3:]
        
#         channel_features = [kur, skw, crest, shape, impulse] 
#         channel_features.extend(top_peaks.tolist())
#         all_channel_features.extend(channel_features)
        
#     return np.array(all_channel_features)

from scipy.spatial.distance import cdist
from sklearn.ensemble import RandomForestClassifier

# --- å¤ç”¨ä¹‹å‰çš„è¾…åŠ©å‡½æ•° ---
# 1. standardize(X)
# 2. calculate_feature_vector(sample_data)

from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from scipy import signal
from scipy.fft import fft, fftfreq
import numpy as np
from tqdm import tqdm

from sklearn.neighbors import NearestNeighbors
from sklearn.preprocessing import StandardScaler
from scipy import signal
from scipy.fft import fft, fftfreq
import numpy as np
from tqdm import tqdm

# --- 1. ä¿æŒé«˜çº§ç‰¹å¾æå–å‡½æ•°ä¸å˜ ---
def extract_advanced_features(time_series, fs=1000):
    """
    é«˜çº§ç‰¹å¾æå–ï¼šå¤šå°ºåº¦æ—¶é¢‘ç‰¹å¾èåˆ
    """
    # ç¡®ä¿è¾“å…¥æ˜¯ (Time, Channels) æ ¼å¼
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
        
    n_channels = time_series.shape[1]
    all_features = []
    
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        
        # æ—¶åŸŸç‰¹å¾
        time_features = [
            np.mean(signal_data),
            np.std(signal_data),
            np.max(np.abs(signal_data)),
            np.max(signal_data) - np.min(signal_data),
            np.sqrt(np.mean(signal_data**2)),
            np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
        ]
        
        # é¢‘åŸŸç‰¹å¾
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
        dominant_freq = f[np.argmax(Pxx)]
        spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10)
        
        freq_features = [
            dominant_freq,
            spectral_centroid,
            np.max(Pxx),
            np.mean(Pxx),
            np.std(Pxx)
        ]
        
        all_features.extend(time_features + freq_features)
    
    # é€šé“ç›¸å…³æ€§
    correlation_features = []
    for i in range(n_channels):
        for j in range(i+1, n_channels):
            corr = np.corrcoef(time_series[:, i], time_series[:, j])[0, 1]
            correlation_features.append(corr if not np.isnan(corr) else 0)
            
    return np.concatenate([np.array(all_features), np.array(correlation_features)])

# # --- 2. ä¿®æ”¹åçš„æ£€ç´¢å‡½æ•°ï¼ˆå¢åŠ äº† train_labelsï¼‰---
# def find_nearest_neighbors_weighted_feature(train_data, train_labels, test_data, num_neighbors):
#     """
#     ä½¿ç”¨ [é«˜çº§ç‰¹å¾] + [æœ‰ç›‘ç£è‡ªé€‚åº”åŠ æƒ] è¿›è¡Œè¿‘é‚»æœç´¢ã€‚
#     åˆ©ç”¨æ ‡ç­¾ä¿¡æ¯è®¡ç®—ç±»å†…æ–¹å·®ï¼Œç»™ç¨³å®šçš„ç‰¹å¾æ›´é«˜çš„æƒé‡ã€‚
#     """
    
#     # --- æ­¥éª¤ 1: æ‰¹é‡æå–ç‰¹å¾ ---
#     print("Extracting advanced features...")
#     train_features = np.array([extract_advanced_features(seq) for seq in tqdm(train_data, desc="Train Feat")])
#     test_features = np.array([extract_advanced_features(seq) for seq in tqdm(test_data, desc="Test Feat")])

#     # --- æ­¥éª¤ 2: ç‰¹å¾æ ‡å‡†åŒ– ---
#     scaler = StandardScaler()
#     train_features_scaled = scaler.fit_transform(train_features)
#     test_features_scaled = scaler.transform(test_features)
    
#     # --- æ­¥éª¤ 3: è®¡ç®—ç‰¹å¾æƒé‡ (è¿™æ˜¯åŠ äº†æ ‡ç­¾åçš„æ ¸å¿ƒæå‡) ---
#     print("Calculating supervised feature weights...")
#     unique_classes = np.unique(train_labels)
#     n_features = train_features_scaled.shape[1]
    
#     # åˆå§‹åŒ–æƒé‡ç´¯åŠ å™¨
#     feature_weights = np.zeros(n_features)
    
#     # å¯¹æ¯ä¸ªç±»åˆ«ï¼Œè®¡ç®—ç‰¹å¾çš„ç¨³å®šæ€§ï¼ˆæ–¹å·®çš„å€’æ•°ï¼‰
#     for label in unique_classes:
#         # æ‰¾åˆ°å±äºè¯¥ç±»çš„æ ·æœ¬
#         class_mask = (train_labels == label)
#         class_data = train_features_scaled[class_mask]
        
#         if len(class_data) > 1:
#             # è®¡ç®—ç±»å†…æ–¹å·®
#             class_var = np.var(class_data, axis=0)
#             # æ–¹å·®è¶Šå°ï¼Œç‰¹å¾è¶Šé‡è¦ã€‚åŠ  1e-5 é˜²æ­¢é™¤ä»¥0
#             weight = 1.0 / (class_var + 1e-5)
#             feature_weights += weight
    
#     # å–å¹³å‡å¹¶å½’ä¸€åŒ–æƒé‡åˆ° [0, 1]
#     feature_weights = feature_weights / len(unique_classes)
#     feature_weights = feature_weights / (np.max(feature_weights) + 1e-10)
    
#     # åº”ç”¨æƒé‡ï¼šé‡è¦çš„ç‰¹å¾è¢«æ”¾å¤§ï¼Œå™ªå£°ç‰¹å¾è¢«ç¼©å°
#     print("Applying feature weights...")
#     train_weighted = train_features_scaled * feature_weights
#     test_weighted = test_features_scaled * feature_weights
    
#     # --- æ­¥éª¤ 4: æœ€è¿‘é‚»æœç´¢ ---
#     print(f"Searching for {num_neighbors} nearest neighbors in weighted space...")
#     nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='auto', metric='euclidean', n_jobs=-1)
#     nbrs.fit(train_weighted)
    
#     distances, indices = nbrs.kneighbors(test_weighted)

#     # --- æ­¥éª¤ 5: æ ¼å¼åŒ–è¾“å‡º ---
#     results = []
#     for test_index in range(len(test_data)):
#         results.append({
#             "test_index": test_index, 
#             "neighbors": indices[test_index].tolist()
#         })

#     return results

def find_nearest_neighbors_weighted_feature(train_data, train_labels, test_data, num_neighbors):
    # --- æ­¥éª¤ 1 & 2: æå–ç‰¹å¾å¹¶æ ‡å‡†åŒ– (ä¿æŒä¸å˜) ---
    print("Extracting advanced features...")
    train_features = np.array([extract_advanced_features(seq) for seq in tqdm(train_data, desc="Train Feat")])
    test_features = np.array([extract_advanced_features(seq) for seq in tqdm(test_data, desc="Test Feat")])
    
    scaler = StandardScaler()
    train_features_scaled = scaler.fit_transform(train_features)
    test_features_scaled = scaler.transform(test_features)
    
    # --- æ­¥éª¤ 3: [ä¿®æ”¹è¿™é‡Œ] åˆ æ‰æ‰€æœ‰æƒé‡è®¡ç®—ï¼Œç›´æ¥æœç´¢ ---
    print(f"Searching for {num_neighbors} nearest neighbors using Euclidean distance...")
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='auto', metric='euclidean', n_jobs=-1)
    # ç›´æ¥ç”¨æ ‡å‡†åŒ–åçš„ç‰¹å¾ fitï¼Œä¸å†ä¹˜ä»¥æƒé‡
    nbrs.fit(train_features_scaled)
    distances, indices = nbrs.kneighbors(test_features_scaled)

    # --- æ­¥éª¤ 5: æ ¼å¼åŒ–è¾“å‡º (ä¿æŒä¸å˜) ---
    results = [{"test_index": i, "neighbors": indices[i].tolist()} for i in range(len(test_data))]
    return results


def neighbor_find(dataset, 
                  train_work_condition_nums,
                  test_work_condition_num,
                  neighbor_num,
                  dist_map={'FIW': find_nearest_neighbors_weighted_feature}
): 
    """
    æŸ¥æ‰¾æœ€è¿‘é‚»ï¼Œå¹¶æ”¯æŒè·³è¿‡ä¸€ä¸ªæˆ–å¤šä¸ªç‰¹å®šæ ‡ç­¾çš„æ•°æ®ã€‚
    
    Args:
        skip_labels (list, optional): è¦è·³è¿‡çš„æ ‡ç­¾åˆ—è¡¨, e.g., ['G3', 'G5']. é»˜è®¤ä¸º None.
    """
    
    # --- åŠ è½½æ‰€æœ‰æ•°æ®ï¼ŒåŒ…æ‹¬æ ‡ç­¾ ---
    print(f"Loading data for dataset: {dataset}")
    
    # æ ¸å¿ƒæ”¹åŠ¨ï¼šå¾ªç¯è¯»å–å¤šä¸ªå·¥å†µå¹¶åˆå¹¶
    train_x_list = [np.load(f'few_shot_test/data/{dataset}/WC{wc}/X_train.npy', mmap_mode='c') for wc in train_work_condition_nums]
    train_y_list = [np.load(f'few_shot_test/data/{dataset}/WC{wc}/y_train.npy', mmap_mode='c') for wc in train_work_condition_nums]
    
    full_train_data = np.concatenate(train_x_list, axis=0)
    full_train_labels = np.concatenate(train_y_list, axis=0)
    
    # æµ‹è¯•é›†åŠ è½½ä¿æŒä¸å˜ï¼ˆå‡è®¾æµ‹è¯•é›†ä¾ç„¶æ˜¯å•ä¸ªå·¥å†µï¼‰
    
    full_test_data = np.load(f'few_shot_test/data/{dataset}/WC{test_work_condition_num}/X_valid.npy', mmap_mode='c')
    full_test_labels = np.load(f'few_shot_test/data/{dataset}/WC{test_work_condition_num}/y_valid.npy', mmap_mode='c')
    
    print(f"Original train size: {len(full_train_data)}")
    print(f"Original test size: {len(full_test_data)}")
    
    train_data = full_train_data
    test_data = full_test_data

    train_tag = "_".join(map(str, train_work_condition_nums))

    # --- åç»­é€»è¾‘ä¸å˜ ---
    for name, func in dist_map.items():
        output_dir = f'few_shot_test/data_index/{dataset}/test_WC{test_work_condition_num}_train_WCs{train_tag}/{name}_dist'
        os.makedirs(output_dir, exist_ok=True)
        print(f"\nCalculating neighbors using {name}...")
        
        for j in range(neighbor_num, neighbor_num + 1):
            print(f"  - Finding {j} nearest neighbors...")
            # åªæœ‰å½“ func æ˜¯è¿™ä¸ªå¸¦æƒé‡çš„å‡½æ•°æ—¶ï¼Œæ‰ä¼  labelï¼Œæˆ–è€…ç»Ÿä¸€éƒ½ä¼ 
            # è¿™é‡Œå‡è®¾ä½ çš„ dist_map é‡Œåªæœ‰è¿™ä¸€ä¸ªå‡½æ•°ï¼Œæˆ–è€…å…¶ä»–å‡½æ•°ä¹Ÿé€‚é…äº†å‚æ•°
            result = func(train_data, full_train_labels, test_data, num_neighbors=j)
            
            output_path = f'{output_dir}/nearest_{j}_neighbors.json'
            with open(output_path, 'w') as f:
                json.dump(result, f, indent=4)
            print(f"    -> Saved results to {output_path}")

def generate_json(dataset):
    # dataset='FingerMovements'
    for work_condition in range(1,10):
        dataset_work_condition=f'{dataset}_WC{work_condition}'
        x_train=np.load(f'few_shot_test/data/{dataset}/WC{work_condition}/X_train.npy',mmap_mode='c')
        x_valid=np.load(f'few_shot_test/data/{dataset}/WC{work_condition}/X_valid.npy',mmap_mode='c')
        y_train=np.load(f'few_shot_test/data/{dataset}/WC{work_condition}/y_train.npy',mmap_mode='c')
        y_valid=np.load(f'few_shot_test/data/{dataset}/WC{work_condition}/y_valid.npy',mmap_mode='c')
        train_index=[]
        test_index=[]

        for i in range(x_train.shape[0]):train_index.append({'index':i,'label':y_train[i]})
        for i in range(x_valid.shape[0]):test_index.append({'index':i,'label':y_valid[i]})

        os.makedirs(f'few_shot_test/data/index/{dataset}/WC{work_condition}', exist_ok=True)
        with open(f'few_shot_test/data/index/{dataset}/WC{work_condition}/train_index.json','w') as f:
            json.dump(train_index,f)
        with open(f'few_shot_test/data/index/{dataset}/WC{work_condition}/test_index.json','w') as f:
            json.dump(test_index,f)


def load_labels_as_map(label_file_path):
    """
    åŠ è½½æ ‡ç­¾ JSON æ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªæ˜“äºæŸ¥æ‰¾çš„å­—å…¸ï¼ˆæ˜ å°„ï¼‰ã€‚
    
    Args:
        label_file_path (str): æ ‡ç­¾æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        dict: ä¸€ä¸ªä» index (int) åˆ° label (str) çš„æ˜ å°„, e.g., {0: 'G0', 1: 'G0', ...}
              å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯ï¼Œè¿”å› Noneã€‚
    """
    try:
        with open(label_file_path, 'r', encoding='utf-8') as f:
            label_list = json.load(f)
        
        # å°† [{"index": 0, "label": "G0"}, ...] è½¬æ¢ä¸º {0: "G0", ...}
        label_map = {item['index']: item['label'] for item in label_list}
        return label_map
    except FileNotFoundError:
        print(f"[ERROR] æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {label_file_path}")
    except (json.JSONDecodeError, KeyError) as e:
        print(f"[ERROR] æ ‡ç­¾æ–‡ä»¶æ ¼å¼é”™è¯¯. éœ€è¦ 'index' å’Œ 'label' é”®. é”™è¯¯: {e}")
    return None


def calculate_retrieval_accuracy(retrieval_results_path, test_labels_path, train_labels_path):
    """
    è®¡ç®—æœ€è¿‘é‚»æ£€ç´¢çš„å‡†ç¡®åº¦ï¼ˆçº¯åº¦ï¼‰ã€‚

    Args:
        retrieval_results_path (str): æ£€ç´¢ç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
        test_labels_path (str): æµ‹è¯•é›†çœŸå®æ ‡ç­¾çš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
        train_labels_path (str): è®­ç»ƒé›†çœŸå®æ ‡ç­¾çš„ JSON æ–‡ä»¶è·¯å¾„ (ç”¨äºæŸ¥æ‰¾é‚»å±…çš„æ ‡ç­¾)ã€‚
    """
    
    # 1. åŠ è½½æ‰€æœ‰å¿…è¦çš„æ ‡ç­¾æ•°æ®
    test_label_map = load_labels_as_map(test_labels_path)
    train_label_map = load_labels_as_map(train_labels_path)
    
    try:
        with open(retrieval_results_path, 'r', encoding='utf-8') as f:
            retrieval_data = json.load(f)
    except FileNotFoundError:
        print(f"[ERROR] æ£€ç´¢ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {retrieval_results_path}")
        return
    except json.JSONDecodeError as e:
        print(f"[ERROR] æ£€ç´¢ç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯. é”™è¯¯: {e}")
        return

    if not test_label_map or not train_label_map:
        print("æ— æ³•ç»§ç»­è¯„ä¼°ï¼Œå› ä¸ºæ ‡ç­¾æ–‡ä»¶åŠ è½½å¤±è´¥ã€‚")
        return

    all_purities = []
    
    # 2. éå†æ¯ä¸€ä¸ªæµ‹è¯•æ ·æœ¬çš„æ£€ç´¢ç»“æœ
    for result_item in retrieval_data:
        test_index = result_item.get('test_index')
        neighbor_indices = result_item.get('neighbors', [])
        
        if test_index is None or not neighbor_indices:
            print(f"[WARNING] è·³è¿‡ test_index {test_index}ï¼Œå› ä¸ºæ•°æ®ä¸å®Œæ•´ã€‚")
            continue
            
        # è·å–å½“å‰æµ‹è¯•æ ·æœ¬çš„çœŸå®æ ‡ç­¾
        true_test_label = test_label_map.get(test_index)
        if true_test_label is None:
            print(f"[WARNING] åœ¨æ ‡ç­¾æ–‡ä»¶ä¸­æ‰¾ä¸åˆ° test_index {test_index} çš„çœŸå®æ ‡ç­¾ã€‚")
            continue
            
        # 3. è®¡ç®—é‚»å±…çº¯åº¦
        correct_neighbors = 0
        for neighbor_idx in neighbor_indices:
            # ä»è®­ç»ƒé›†æ ‡ç­¾æ˜ å°„ä¸­æŸ¥æ‰¾é‚»å±…çš„æ ‡ç­¾
            neighbor_label = train_label_map.get(neighbor_idx)
            
            if neighbor_label is not None and neighbor_label == true_test_label:
                correct_neighbors += 1
        
        # çº¯åº¦ = (ä¸æµ‹è¯•æ ·æœ¬åŒç±»çš„é‚»å±…æ•°) / (æ€»é‚»å±…æ•°)
        purity = correct_neighbors / len(neighbor_indices)
        all_purities.append(purity)
        
    # 4. è®¡ç®—å¹¶æ‰“å°æ€»ä½“ç»“æœ
    if not all_purities:
        print("[ERROR] æ²¡æœ‰å¯ä¾›è¯„ä¼°çš„æœ‰æ•ˆæ£€ç´¢ç»“æœã€‚")
        return 0.0
    
    mean_accuracy = np.mean(all_purities) * 100
    
    print("\n" + "="*50)
    print(f"[INFO] æœ€è¿‘é‚»æ£€ç´¢ç²¾åº¦è¯„ä¼°æŠ¥å‘Š")
    print(f"[INFO] - æ£€ç´¢æ–‡ä»¶: {os.path.basename(retrieval_results_path)}")
    print("="*50)
    print(f"[INFO] æ€»è®¡è¯„ä¼°çš„æµ‹è¯•æ ·æœ¬æ•°: {len(all_purities)}")
    print(f"[INFO] å¹³å‡æ£€ç´¢ç²¾åº¦ (Mean Purity @ k): {mean_accuracy:.2f}%")
    print("="*50)
    print(f"[INFO] (è¯¥æŒ‡æ ‡è¡¡é‡çš„æ˜¯ï¼šå¯¹äºä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå…¶æ‰¾åˆ°çš„é‚»å±…æœ‰å¤šå¤§æ¦‚ç‡ä¸å®ƒè‡ªå·±æ˜¯åŒä¸€ç±»åˆ«)")
    
    return mean_accuracy


# weight_DTW=0.1
# weight_feature=0.9
# def pipeline():
#     generate_json(dataset=dataset)
#     neighbor_find(dataset=dataset,
#                     train_work_condition_num=train_work_condition_num,
#                     test_work_condition_num=test_work_condition_num,
#                     dist_map = dist_map,
#                     neighbor_num = neighbor_num,
#                     skip_labels = None,)
#     calculate_retrieval_accuracy(retrieval_results_path=os.path.join("data_index", dataset, f"test_WC{test_work_condition_num}_train_WC{train_work_condition_num}",f"{list(dist_map.keys())[0]}_dist", f'nearest_{neighbor_num}_neighbors.json'),test_labels_path=os.path.join("data", "index",dataset,f"WC{test_work_condition_num}","test_index.json"), train_labels_path=os.path.join("data", "index", dataset,f"WC{train_work_condition_num}", "train_index.json"))

def pipeline(dataset, train_nums, test_num, dist_map, neighbor_num):
    # 1. ç”ŸæˆåŸºç¡€ç´¢å¼• (å¦‚æœéœ€è¦)
    generate_json(dataset=dataset)
    
    # 2. å¯»æ‰¾è¿‘é‚» (æ³¨æ„è¿™é‡Œä¼ çš„æ˜¯åˆ—è¡¨ train_nums)
    neighbor_find(dataset=dataset,
                  train_work_condition_nums=train_nums,
                  test_work_condition_num=test_num,
                  dist_map=dist_map,
                  neighbor_num=neighbor_num)
    
    # 3. æ„é€ è·¯å¾„æ ‡è¯† (ä¾‹å¦‚ [1,2,3] -> "1_2_3")
    train_tag = "_".join(map(str, train_nums))
    results_path = os.path.join("few_shot_test/data_index", dataset, f"test_WC{test_num}_train_WCs{train_tag}", 
                                f"{list(dist_map.keys())[0]}_dist", f'nearest_{neighbor_num}_neighbors.json')
    
    # 4. åˆå¹¶è®­ç»ƒé›†æ ‡ç­¾ (æ ¸å¿ƒæ”¹åŠ¨ï¼šå› ä¸ºåˆå¹¶åçš„è®­ç»ƒé›†ç´¢å¼•æ˜¯è¿ç»­çš„ï¼Œéœ€è¦æ‰‹åŠ¨åˆå¹¶å­—å…¸)
    merged_train_labels = {}
    current_offset = 0
    for wc in train_nums:
        path = os.path.join("few_shot_test/data", "index", dataset, f"WC{wc}", "train_index.json")
        with open(path, 'r') as f:
            labels = json.load(f)
            for item in labels:
                # å°†è¯¥å·¥å†µçš„æ ‡ç­¾å­˜å…¥åˆå¹¶å­—å…¸ï¼Œé”®ä¸ºå…¨å±€åç§»åçš„ç´¢å¼•
                merged_train_labels[current_offset] = item['label']
                current_offset += 1
    
    # 5. åŠ è½½æµ‹è¯•é›†æ ‡ç­¾
    test_labels_path = os.path.join("few_shot_test/data", "index", dataset, f"WC{test_num}", "test_index.json")
    
    # 6. è®¡ç®—å‡†ç¡®ç‡ (è¿™é‡Œéœ€è¦ç¨å¾®ä¿®æ”¹ calculate_retrieval_accuracy ä½¿å…¶æ”¯æŒç›´æ¥ä¼ å­—å…¸ï¼Œæˆ–è€…å¦‚ä¸‹å¿«æ·å¤„ç†)
    # ä¸ºäº†æœ€å°åŒ–æ”¹åŠ¨ï¼Œæˆ‘ä»¬ä¸´æ—¶å†™ä¸€ä¸ªåˆå¹¶åçš„json
    temp_train_labels_path = "temp_merged_train_labels.json"
    with open(temp_train_labels_path, 'w') as f:
        json.dump([{"index": k, "label": v} for k, v in merged_train_labels.items()], f)
        
    acc=calculate_retrieval_accuracy(retrieval_results_path=results_path,
                                 test_labels_path=test_labels_path,
                                 train_labels_path=temp_train_labels_path)

    return acc
    
# if __name__ == "__main__":
        
#     dataset='BJTU-gearbox'
#     dist_map = {'FIW': find_nearest_neighbors_weighted_feature}
#     neighbor_num = 15
#     train_work_condition_num=1
#     test_work_condition_num=2
#     pipeline()

if __name__ == "__main__":
    import datetime # ç¡®ä¿å¯¼å…¥ datetime
    
    dataset = 'BJTU-gearbox'
    # dataset = 'BJTU-motor'
    # dataset = 'BJTU-leftaxlebox'
    dist_map_name = 'FIW'
    dist_map = {dist_map_name: find_nearest_neighbors_weighted_feature}
    neighbor_num = 3
    all_wcs = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    
    # å®šä¹‰è®­ç»ƒåœºæ™¯
    # all_train_scenarios = [
    #     [1, 4],
    #     [1, 4, 7],
    #     [1, 2, 3, 4, 6],
    #     [1, 2, 3, 4, 5, 6, 7]
    # ]
    
    # all_train_scenarios = [
    #     [1, 2],
    #     [1, 2, 3],
    #     [1, 2, 3, 4, 5],
    #     [1, 2, 3, 4, 5, 6, 7]
    # ]
    
    all_train_scenarios = [
        [1, 2, 3,]
    ]
    
    # ç”¨äºæ”¶é›†æ‰€æœ‰å®éªŒç»“æœçš„åˆ—è¡¨
    experiment_logs = []
    
    # --- å¼€å§‹å¤§å¾ªç¯ ---
    for train_nums in all_train_scenarios:
        test_wcs = [wc for wc in all_wcs if wc not in train_nums]
        
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¤§å®éªŒå¯åŠ¨ï¼šè®­ç»ƒé›†ç»„åˆ = {train_nums}")
        print(f"{'='*60}")
        
        scenario_accuracies = []
        
        for test_wc in test_wcs:
            print(f"\n>>> [å½“å‰é…ç½®] è®­ç»ƒ: {train_nums} | æµ‹è¯•: WC{test_wc}")
            # è·å–å‡†ç¡®ç‡
            acc = pipeline(dataset, train_nums, test_wc, dist_map, neighbor_num)
            
            # è®°å½•å•æ¬¡ç»“æœ
            log_str = f"Train: {train_nums} | Test: WC{test_wc} | Accuracy: {acc:.2f}%"
            experiment_logs.append(log_str)
            scenario_accuracies.append(acc)
        
        # è®°å½•è¯¥åœºæ™¯çš„å¹³å‡å‡†ç¡®ç‡
        avg_acc = np.mean(scenario_accuracies) if scenario_accuracies else 0
        experiment_logs.append(f"--- Scenario Average (Train {train_nums}): {avg_acc:.2f}% ---\n")

    # --- å®éªŒç»“æŸï¼Œä¿å­˜æ±‡æ€»ç»“æœ ---
    
    # 1. ç”Ÿæˆæ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "few_shot_test/result/log"
    os.makedirs(log_dir, exist_ok=True)
    filename = f"{dataset}_{dist_map_name}_{timestamp}.txt"
    filepath = os.path.join(log_dir, filename)
    
    # 2. æ„å»ºå®Œæ•´æŠ¥å‘Šå†…å®¹
    final_report = []
    final_report.append("="*60)
    final_report.append(f"å®éªŒæ±‡æ€»æŠ¥å‘Š")
    final_report.append(f"æ—¶é—´: {timestamp}")
    final_report.append(f"æ•°æ®é›†: {dataset}")
    final_report.append(f"è·ç¦»åº¦é‡: {dist_map_name}")
    final_report.append(f"é‚»å±…æ•°: {neighbor_num}")
    final_report.append("="*60 + "\n")
    final_report.extend(experiment_logs)
    
    final_report_str = "\n".join(final_report)
    
    # 3. æ‰“å°å¹¶ä¿å­˜
    print("\n" + "#"*60)
    print("å®éªŒå…¨éƒ¨å®Œæˆï¼æ±‡æ€»ç»“æœå¦‚ä¸‹ï¼š")
    print("#"*60)
    print(final_report_str)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(final_report_str)
        
    print(f"\n[INFO] æ±‡æ€»æ—¥å¿—å·²ä¿å­˜è‡³: {filepath}")