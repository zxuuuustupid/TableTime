import json
import sys
import os
import numpy as np
from tqdm import tqdm
from scipy import signal
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# è·¯å¾„é˜²æŠ¥é”™
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if root_path not in sys.path:
    sys.path.insert(0, root_path)

# --- 1. é…ç½®å‚æ•° ---
DATASET = "BJTU-gearbox"
BASE_PATH = f"few_shot_test/data/{DATASET}"
NUM_NEIGHBORS = 1  # æ‰¾æœ€è¿‘çš„1ä¸ª

# --- 2. æ ¸å¿ƒå·¥å…·å‡½æ•° ---

def extract_advanced_features(time_series, fs=1000):
    """(ä¿æŒä¸å˜)"""
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
    n_channels = time_series.shape[1]
    all_features = []
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        time_features = [
            np.mean(signal_data), np.std(signal_data), np.max(np.abs(signal_data)),
            np.max(signal_data) - np.min(signal_data), np.sqrt(np.mean(signal_data**2)),
            np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
        ]
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
        freq_features = [f[np.argmax(Pxx)], np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10), np.max(Pxx), np.mean(Pxx), np.std(Pxx)]
        all_features.extend(time_features + freq_features)
    correlation_features = [np.corrcoef(time_series[:, i], time_series[:, j])[0, 1] 
                            for i in range(n_channels) for j in range(i+1, n_channels)]
    return np.concatenate([np.array(all_features), np.nan_to_num(np.array(correlation_features))])

def load_labels_as_map(label_file_path):
    try:
        with open(label_file_path, 'r', encoding='utf-8') as f:
            label_list = json.load(f)
        return {item['index']: item['label'] for item in label_list}
    except Exception:
        return {}

# def calculate_retrieval_accuracy(retrieval_results_path, test_labels_path, train_labels_path):
#     """(ä¿æŒä¸å˜)"""
#     test_label_map = load_labels_as_map(test_labels_path)
#     train_label_map = load_labels_as_map(train_labels_path)
    
#     with open(retrieval_results_path, 'r', encoding='utf-8') as f:
#         retrieval_data = json.load(f)

#     all_purities = []
#     class_stats = {}

#     for result_item in retrieval_data:
#         test_index = result_item.get('test_index')
#         neighbor_indices = result_item.get('neighbors', [])
        
#         true_test_label = test_label_map.get(test_index)
        
#         correct_neighbors = 0
#         for neighbor_idx in neighbor_indices:
#             neighbor_label = train_label_map.get(neighbor_idx)
#             if neighbor_label == true_test_label:
#                 correct_neighbors += 1
        
#         if len(neighbor_indices) > 0:
#             purity = correct_neighbors / len(neighbor_indices)
#             all_purities.append(purity)

#             # [æ–°å¢] 2. åœ¨å¾ªç¯å†…ç´¯è®¡æ¯ä¸ªæ ‡ç­¾çš„çº¯åº¦å’Œè®¡æ•°
#             if true_test_label not in class_stats:
#                 class_stats[true_test_label] = [0.0, 0]
            
#             class_stats[true_test_label][0] += purity  # ç´¯åŠ çº¯åº¦
#             class_stats[true_test_label][1] += 1       # ç´¯åŠ æ•°é‡
            
#     mean_accuracy = np.mean(all_purities) * 100
    
#     # [æ–°å¢] 3. æ‰“å°è¯¦ç»†çš„åˆ†é¡¹ç»“æœ
#     print(f"\n{'='*20} è¯¦ç»†åˆ†ç±»ç»“æœ {'='*20}")
#     print(f"{'æ•…éšœç±»å‹':<10} | {'å‡†ç¡®ç‡'}")
#     print("-" * 35)
    
#     # æŒ‰æ ‡ç­¾å­—æ¯é¡ºåºæ’åºæ‰“å°
#     for label in sorted(class_stats.keys()):
#         total_purity, count = class_stats[label]
#         acc = (total_purity / count) * 100 if count > 0 else 0
#         print(f"{label:<14} | {acc:.2f}%")
        
#     print("-" * 35)
#     print(f"{'æ€»ä½“å¹³å‡':<14} | {mean_accuracy:.2f}%")
#     print("=" * 46 + "\n")

#     return mean_accuracy



def calculate_retrieval_accuracy(retrieval_results_path, test_labels_path, train_labels_path):
    """(å¸¦è°ƒè¯•ç‰ˆ)"""
    print(f"\n--- å¼€å§‹è®¡ç®—å‡†ç¡®ç‡ ---")
    
    # 1. åŠ è½½æ ‡ç­¾
    test_label_map = load_labels_as_map(test_labels_path)
    train_label_map = load_labels_as_map(train_labels_path)
    
    # [è°ƒè¯•] æ‰“å°æ ‡ç­¾Mapçš„åŸºæœ¬ä¿¡æ¯
    print(f"[DEBUG] æµ‹è¯•é›†æ ‡ç­¾æ•°é‡: {len(test_label_map)}")
    print(f"[DEBUG] è®­ç»ƒé›†æ ‡ç­¾æ•°é‡: {len(train_label_map)}")
    
    # [è°ƒè¯•] æ‰“å°å‰5ä¸ªæµ‹è¯•é›†ç´¢å¼•ï¼Œçœ‹çœ‹é•¿ä»€ä¹ˆæ ·
    first_5_keys = list(test_label_map.keys())[:5]
    print(f"[DEBUG] æµ‹è¯•é›†ç´¢å¼•ç¤ºä¾‹(å‰5ä¸ª): {first_5_keys}")

    with open(retrieval_results_path, 'r', encoding='utf-8') as f:
        retrieval_data = json.load(f)
    
    print(f"[DEBUG] æ£€ç´¢ç»“æœæ¡ç›®æ•°: {len(retrieval_data)}")

    all_purities = []
    class_stats = {}

    for i, result_item in enumerate(retrieval_data):
        test_index = result_item.get('test_index')
        neighbor_indices = result_item.get('neighbors', [])
        
        # è·å–çœŸå®æ ‡ç­¾
        true_test_label = test_label_map.get(test_index)
        
        # [å…³é”®è°ƒè¯•] å¦‚æœæ‰¾ä¸åˆ°æ ‡ç­¾ï¼Œç«‹åˆ»æŠ¥é”™å¹¶æ‰“å°è¯¦æƒ…ï¼Œè€Œä¸æ˜¯è·³è¿‡
        if true_test_label is None:
            print(f"\n[CRITICAL ERROR] åœ¨ç¬¬ {i} æ¡æ£€ç´¢ç»“æœä¸­å‘ç°å¼‚å¸¸ï¼")
            print(f"  - æ£€ç´¢ç»“æœä¸­çš„ test_index: {test_index} (ç±»å‹: {type(test_index)})")
            print(f"  - test_label_map ä¸­æ˜¯å¦å­˜åœ¨è¯¥Key? {test_index in test_label_map}")
            # å°è¯•è½¬æ¢ç±»å‹å†æŸ¥ä¸€æ¬¡ï¼Œæ’é™¤ int/str ä¸åŒ¹é…çš„é—®é¢˜
            print(f"  - å°è¯•è½¬ä¸º int æŸ¥æ‰¾: {test_label_map.get(int(test_index))}")
            print(f"  - å°è¯•è½¬ä¸º str æŸ¥æ‰¾: {test_label_map.get(str(test_index))}")
            raise ValueError(f"æ— æ³•æ‰¾åˆ° test_index={test_index} çš„çœŸå®æ ‡ç­¾ï¼è¯·æ£€æŸ¥ä¸Šè¿°è°ƒè¯•ä¿¡æ¯ã€‚")

        correct_neighbors = 0
        for neighbor_idx in neighbor_indices:
            neighbor_label = train_label_map.get(neighbor_idx)
            if neighbor_label == true_test_label:
                correct_neighbors += 1
        
        if len(neighbor_indices) > 0:
            purity = correct_neighbors / len(neighbor_indices)
            all_purities.append(purity)

            # ç»Ÿè®¡åˆ†é¡¹ç»“æœ
            if true_test_label not in class_stats:
                class_stats[true_test_label] = [0.0, 0]
            class_stats[true_test_label][0] += purity
            class_stats[true_test_label][1] += 1
            
    mean_accuracy = np.mean(all_purities) * 100
    
    # æ‰“å°è¯¦ç»†ç»“æœ
    print(f"\n{'='*20} è¯¦ç»†åˆ†ç±»ç»“æœ {'='*20}")
    print(f"{'æ•…éšœç±»å‹':<10} | {'å‡†ç¡®ç‡'}")
    print("-" * 35)
    
    # æ­¤æ—¶ class_stats é‡Œç»å¯¹æ²¡æœ‰ Noneï¼Œå¯ä»¥æ”¾å¿ƒæ’åº
    for label in sorted(class_stats.keys()):
        total_purity, count = class_stats[label]
        acc = (total_purity / count) * 100 if count > 0 else 0
        print(f"{label:<14} | {acc:.2f}%")
        
    print("-" * 35)
    print(f"{'æ€»ä½“å¹³å‡':<14} | {mean_accuracy:.2f}%")
    print("=" * 46 + "\n")

    return mean_accuracy
# --- 3. æ ¸å¿ƒæ£€ç´¢å‡½æ•° (æ”¹å›åŸå neighbor_find å¹¶æ”¯æŒæ··åˆé€»è¾‘) ---

def neighbor_find_mixed(dataset, 
                        source_wcs, 
                        target_wc, 
                        target_n_shots, 
                        neighbor_num, 
                        dist_map):
    """
    ä¸“é—¨å¤„ç†æ··åˆå·¥å†µï¼šSource(å…¨é‡) + Target(å°æ ·æœ¬)
    å…³é”®ï¼šå®Œå…¨ä¿ç•™åŸæ¥çš„æ–‡ä»¶å¤¹å‘½åé€»è¾‘
    """
    
    # æ„é€ åŸæ¥çš„ train_nums åˆ—è¡¨æ¦‚å¿µï¼Œç”¨äºå‘½å
    # æ³¨æ„é¡ºåºï¼šå…ˆæ”¾ Sourceï¼Œæœ€åæ”¾ Target
    train_nums = source_wcs + [target_wc]
    train_tag = "_".join(map(str, train_nums))
    
    print(f"æ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ® (Tag: {train_tag})...")

    # --- A. åŠ¨æ€æ„å»ºè®­ç»ƒé›†ä¸æ ‡ç­¾ ---
    # æˆ‘ä»¬éœ€è¦åœ¨å†…å­˜é‡ŒåŒæ—¶æ„å»º features å’Œ merged_train_labelsï¼Œç¡®ä¿ç´¢å¼•ä¸€ä¸€å¯¹åº”
    
    train_feats_list = []
    merged_train_labels = {} # {0: 'G0', 1: 'G0', ...}
    global_idx = 0
    
    # 1. åŠ è½½ Source WCs (å…¨é‡)
    for wc in source_wcs:
        path = os.path.join(BASE_PATH, f"WC{wc}")
        x = np.load(os.path.join(path, 'X_train.npy'), mmap_mode='c').astype(np.float32)
        y = np.load(os.path.join(path, 'y_train.npy'), mmap_mode='c').astype(str)
        
        # æå–ç‰¹å¾
        print(f"  -> æå– Source WC{wc} ç‰¹å¾...")
        feats = np.array([extract_advanced_features(s) for s in tqdm(x, leave=False)])
        train_feats_list.append(feats)
        
        # è®°å½•æ ‡ç­¾
        for label in y:
            merged_train_labels[global_idx] = label
            global_idx += 1
            
    # 2. åŠ è½½ Target WC (å°æ ·æœ¬)
    path = os.path.join(BASE_PATH, f"WC{target_wc}")
    x_t = np.load(os.path.join(path, 'X_train.npy'), mmap_mode='c').astype(np.float32)
    y_t = np.load(os.path.join(path, 'y_train.npy'), mmap_mode='c').astype(str)
    
    # ç­›é€‰å‰ n_shots
    unique_labels = np.unique(y_t)
    sel_indices = []
    for lbl in unique_labels:
        idxs = np.where(y_t == lbl)[0]
        sel_indices.extend(idxs[:target_n_shots])
    
    x_t_few = x_t[sel_indices]
    y_t_few = y_t[sel_indices]
    
    print(f"  -> æå– Target WC{target_wc} å°æ ·æœ¬ç‰¹å¾ ({len(x_t_few)}ä¸ª)...")
    t_feats = np.array([extract_advanced_features(s) for s in x_t_few])
    train_feats_list.append(t_feats)
    
    for label in y_t_few:
        merged_train_labels[global_idx] = label
        global_idx += 1
    
    # åˆå¹¶è®­ç»ƒç‰¹å¾
    X_train_std = np.concatenate(train_feats_list, axis=0)
    
    # --- B. åŠ è½½æµ‹è¯•é›† (Target WC Valid) ---
    path = os.path.join(BASE_PATH, f"WC{target_wc}")
    x_valid = np.load(os.path.join(path, 'X_valid.npy'), mmap_mode='c').astype(np.float32)
    # y_valid æˆ‘ä»¬åé¢å•ç‹¬å¤„ç†ç”Ÿæˆ test_index.json ç”¨
    
    print(f"  -> æå–æµ‹è¯•é›† WC{target_wc} ç‰¹å¾...")
    X_test_std = np.array([extract_advanced_features(s) for s in tqdm(x_valid, leave=False)])
    
    # --- C. æ ‡å‡†åŒ–ä¸æ£€ç´¢ ---
    # ä½ çš„ç®—æ³•ç°åœ¨æ˜¯çº¯æ¬§æ°è·ç¦»ï¼Œæ‰€ä»¥è¦æŠŠç‰¹å¾æå–åçš„ X_train_std æ”¾å…¥ Scaler
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train_std)
    X_test_scaled = scaler.transform(X_test_std)
    
    # éå† dist_map (è™½ç„¶ç°åœ¨é€»è¾‘å›ºå®šäº†ï¼Œä½†ä¸ºäº†ä¿æŒæ–‡ä»¶å¤¹ç»“æ„)
    for dist_name, _ in dist_map.items():
        # [å…³é”®] ä¸¥æ ¼ä¿æŒåŸæœ¬çš„æ–‡ä»¶å¤¹å‘½åæ ¼å¼ï¼
        output_dir = os.path.join("few_shot_test", "data_index", dataset, 
                                  f"test_WC{target_wc}_train_WCs{train_tag}", 
                                  f"{dist_name}_dist")
        os.makedirs(output_dir, exist_ok=True)
        
        print(f"\nè®¡ç®—è¿‘é‚» ({dist_name})...")
        nbrs = NearestNeighbors(n_neighbors=neighbor_num, metric='euclidean', n_jobs=-1)
        nbrs.fit(X_train_scaled)
        _, indices = nbrs.kneighbors(X_test_scaled)
        
        results = []
        for i in range(len(indices)):
            results.append({
                "test_index": i,
                "neighbors": indices[i].tolist()
            })
            
        output_path = os.path.join(output_dir, f'nearest_{neighbor_num}_neighbors.json')
        with open(output_path, 'w') as f:
            json.dump(results, f, indent=4)
        print(f"    -> ç»“æœå·²ä¿å­˜: {output_path}")
        
        # è¿”å›è·¯å¾„ä¾›ä¸‹ä¸€æ­¥ä½¿ç”¨
        return output_path, merged_train_labels

# --- 4. ç®¡é“å‡½æ•° (é€»è¾‘å¤åŸ) ---

def pipeline(dataset, source_wcs, target_wc, target_n_shots, dist_map, neighbor_num):
    # 1. ç”ŸæˆåŸæœ¬çš„ test_index.json (æµ‹è¯•é›†ç´¢å¼•)
    # å› ä¸ºæµ‹è¯•é›†æ˜¯å›ºå®šçš„ WC{target_wc} çš„ X_validï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥ç”¨åŸæ¥çš„ generate_json é€»è¾‘
    # æˆ–è€…ä¸ºäº†ç®€å•ï¼Œç›´æ¥åœ¨è¿™é‡Œå†™ä¸€ä¸ªä¸´æ—¶çš„
    path = os.path.join(BASE_PATH, f"WC{target_wc}")
    y_valid = np.load(os.path.join(path, 'y_valid.npy'), mmap_mode='c').astype(str)
    test_index_json = [{"index": i, "label": l} for i, l in enumerate(y_valid)]
    
    temp_test_labels_path = "temp_test_labels.json"
    with open(temp_test_labels_path, 'w') as f:
        json.dump(test_index_json, f)

    # 2. æ‰§è¡Œæ£€ç´¢ & è·å–åˆå¹¶åçš„è®­ç»ƒæ ‡ç­¾
    # è¿™é‡Œçš„æ£€ç´¢å‡½æ•°ä¼šè‡ªåŠ¨æŒ‰æ—§æ ¼å¼ä¿å­˜ json æ–‡ä»¶
    results_path, merged_train_labels = neighbor_find_mixed(
        dataset, source_wcs, target_wc, target_n_shots, neighbor_num, dist_map
    )
    
    # 3. ä¿å­˜ä¸´æ—¶çš„åˆå¹¶è®­ç»ƒæ ‡ç­¾ (å› ä¸ºæ˜¯æ··åˆæ•°æ®ï¼Œä¸èƒ½ç”¨ç£ç›˜ä¸ŠåŸæ¥çš„ json)
    temp_train_labels_path = "temp_merged_train_labels.json"
    train_json_list = [{"index": k, "label": v} for k, v in merged_train_labels.items()]
    with open(temp_train_labels_path, 'w') as f:
        json.dump(train_json_list, f)
        
    # 4. è®¡ç®—å‡†ç¡®ç‡
    acc = calculate_retrieval_accuracy(results_path, temp_test_labels_path, temp_train_labels_path)
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶ (å¯é€‰)
    # os.remove(temp_test_labels_path)
    # os.remove(temp_train_labels_path)
    
    return acc

if __name__ == "__main__":
    import datetime
    
    dataset = 'BJTU-gearbox'
    # è¿™é‡Œçš„åå­— FIW ä¼šå†³å®šæ–‡ä»¶å¤¹å« FIW_distï¼Œè™½ç„¶å†…éƒ¨å·²ç»æ˜¯æ¬§æ°è·ç¦»
    dist_map = {'FIW': None} 
    neighbor_num = 3
    
    # é…ç½®å®éªŒ
    target_wc = 8       # ç›®æ ‡æµ‹è¯•å·¥å†µ
    source_wcs = [1, 2, 3, 4, 5, 7, 9]  # æºå·¥å†µ
    target_n_shots = 3
    # ç›®æ ‡å·¥å†µæ··å…¥å‡ ä¸ªæ ·æœ¬
    
    print(f"\n{'='*60}")
    print(f"ğŸš€ å®éªŒå¼€å§‹: Source={source_wcs} + Target=WC{target_wc}({target_n_shots} shot)")
    print(f"{'='*60}")
    
    acc = pipeline(dataset, source_wcs, target_wc, target_n_shots, dist_map, neighbor_num)
    
    print(f"\næœ€ç»ˆå‡†ç¡®ç‡: {acc:.2f}%")
    