import json
import sys
import os
import datetime
import numpy as np
from tqdm import tqdm
from scipy import signal
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors

# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„ä¸Šä¸€å±‚ç›®å½•ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰
root_path = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
if root_path not in sys.path:
    sys.path.insert(0, root_path)

# --- 1. é…ç½®å‚æ•° ---
DATASET = "BJTU-gearbox"
# ç¡®ä¿è¿™é‡Œæ˜¯ä½ å­˜æ”¾æ•°æ®çš„çœŸå®è·¯å¾„
BASE_PATH = f"few_shot_test/data/{DATASET}" 

# æ ¸å¿ƒè®¾ç½®ï¼šæ¯ä¸ªå·¥å†µã€æ¯ä¸ªç±»åˆ«æ”¾å…¥åº“ä¸­çš„æ ·æœ¬æ•°
# å¦‚æœè®¾ä¸º 1ï¼Œå°±æ˜¯ 1-shot (æåº¦ç¨€ç–)ï¼›è®¾ä¸º 3 å°±æ˜¯ 3-shot
N_SHOTS = 3  

# æ‰¾æœ€è¿‘é‚»çš„ä¸ªæ•° (å»ºè®®è®¾ä¸º 1ï¼Œå› ä¸ºæ ·æœ¬å¤ªå°‘äº†)
NUM_NEIGHBORS = 3 

# --- 2. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def extract_advanced_features(time_series, fs=1000):
    """
    é«˜çº§ç‰¹å¾æå–ï¼šæ—¶åŸŸ + é¢‘åŸŸ + é€šé“ç›¸å…³æ€§
    """
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
    n_channels = time_series.shape[1]
    all_features = []
    
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        # æ—¶åŸŸç‰¹å¾
        time_features = [
            np.mean(signal_data), np.std(signal_data), np.max(np.abs(signal_data)),
            np.max(signal_data) - np.min(signal_data), np.sqrt(np.mean(signal_data**2)),
            np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
        ]
        # é¢‘åŸŸç‰¹å¾
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
        freq_features = [f[np.argmax(Pxx)], np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10), np.max(Pxx), np.mean(Pxx), np.std(Pxx)]
        all_features.extend(time_features + freq_features)
    
    # é€šé“ç›¸å…³æ€§
    correlation_features = [np.corrcoef(time_series[:, i], time_series[:, j])[0, 1] 
                            for i in range(n_channels) for j in range(i+1, n_channels)]
    return np.concatenate([np.array(all_features), np.nan_to_num(np.array(correlation_features))])

def load_few_shot_data(wc_id, n_shots):
    """
    [æ ¸å¿ƒä¿®æ”¹] åŠ è½½æ•°æ®ï¼Œå¹¶ä¸¥æ ¼æ¨¡æ‹Ÿå°æ ·æœ¬ (Few-Shot)
    ä»è®­ç»ƒé›†ä¸­ï¼Œæ¯ä¸ªç±»åˆ«åªå–å‰ n_shots ä¸ªæ ·æœ¬
    """
    path = os.path.join(BASE_PATH, f"WC{wc_id}")
    x_train = np.load(os.path.join(path, 'X_train.npy'), mmap_mode='c').astype(np.float32)
    y_train = np.load(os.path.join(path, 'y_train.npy'), mmap_mode='c').astype(str)
    
    # éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†ï¼Œå…¨éƒ¨ä¿ç•™
    x_valid = np.load(os.path.join(path, 'X_valid.npy'), mmap_mode='c').astype(np.float32)
    y_valid = np.load(os.path.join(path, 'y_valid.npy'), mmap_mode='c').astype(str)
    
    # --- åˆ¶ä½œå°æ ·æœ¬è®­ç»ƒé›† ---
    unique_labels = np.unique(y_train)
    selected_indices = []
    
    for label in unique_labels:
        # æ‰¾åˆ°è¯¥æ ‡ç­¾çš„æ‰€æœ‰ç´¢å¼•
        indices = np.where(y_train == label)[0]
        # åªå–å‰ n_shots ä¸ª
        selected_indices.extend(indices[:n_shots])
    
    selected_indices = np.array(selected_indices)
    x_few_shot = x_train[selected_indices]
    y_few_shot = y_train[selected_indices]
    
    return x_few_shot, y_few_shot, x_valid, y_valid

def euclidean_retrieval(train_feat, train_labels, test_feat, test_labels, num_neighbors):
    """
    ä½¿ç”¨æ ‡å‡†åŒ–æ¬§æ°è·ç¦»è¿›è¡Œæ£€ç´¢ (æ— ç›‘ç£ï¼Œä¸ä½¿ç”¨æ ‡ç­¾æƒé‡)
    """
    # 1. æ•´ä½“æ ‡å‡†åŒ– (éå¸¸é‡è¦ï¼šæ¶ˆé™¤ç‰¹å¾é‡çº§å·®å¼‚)
    scaler = StandardScaler()
    train_scaled = scaler.fit_transform(train_feat)
    test_scaled = scaler.transform(test_feat)
    
    # 2. æœ€è¿‘é‚»æœç´¢
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='euclidean', n_jobs=-1)
    nbrs.fit(train_scaled)
    _, indices = nbrs.kneighbors(test_scaled)
    
    # 3. è®¡ç®—å‡†ç¡®ç‡
    total_correct = 0
    total_samples = len(test_labels)
    
    for i in range(total_samples):
        # æ‰¾åˆ°çš„è¿‘é‚»æ ‡ç­¾
        neighbor_labels = train_labels[indices[i]]
        # çœŸå®æ ‡ç­¾
        true_label = test_labels[i]
        
        # ç»Ÿè®¡å‘½ä¸­æ•°
        hits = np.sum(neighbor_labels == true_label)
        total_correct += hits
        
    # è®¡ç®—çº¯åº¦/å‡†ç¡®ç‡
    accuracy = (total_correct / (total_samples * num_neighbors)) * 100
    return accuracy

# --- 3. å®éªŒä¸»æµç¨‹ ---

def run_few_shot_experiment(train_wcs, test_wcs, n_shots):
    print(f"\n{'='*60}")
    print(f"ğŸ§ª å¯åŠ¨å°æ ·æœ¬å®éªŒ (N_SHOTS={n_shots})")
    print(f"ğŸ“š çŸ¥è¯†åº“åŒ…å«å·¥å†µ: {train_wcs}")
    print(f"ğŸ¯ æµ‹è¯•ç›®æ ‡å·¥å†µ: {test_wcs}")
    print(f"{'='*60}")

    # --- A. æ„å»ºå…¨å±€å°æ ·æœ¬çŸ¥è¯†åº“ ---
    print("æ­£åœ¨æ„å»ºçŸ¥è¯†åº“ (æå–ç‰¹å¾)...")
    library_feats = []
    library_labels = []
    
    for wc in train_wcs:
        # åŠ è½½ç»è¿‡ç­›é€‰çš„å°æ ·æœ¬
        x_s, y_s, _, _ = load_few_shot_data(wc, n_shots)
        
        # æå–ç‰¹å¾
        feats = np.array([extract_advanced_features(s) for s in x_s]) # è¿™é‡Œæ•°æ®é‡å¾ˆå°ï¼Œä¸ç”¨tqdmä¹Ÿæ²¡äº‹
        
        library_feats.append(feats)
        library_labels.append(y_s)
        
    # åˆå¹¶æˆå¤§åº“
    X_library = np.concatenate(library_feats, axis=0)
    y_library = np.concatenate(library_labels, axis=0)
    
    print(f"âœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆ: æ€»æ ·æœ¬æ•° {len(X_library)} (æ¥è‡ª {len(train_wcs)} ä¸ªå·¥å†µ)")

    # --- B. é€ä¸ªå·¥å†µè¿›è¡Œæµ‹è¯• ---
    results = []
    
    for test_wc in test_wcs:
        print(f"\n>>> æ­£åœ¨æµ‹è¯•å·¥å†µ WC{test_wc} ...")
        
        # åŠ è½½æµ‹è¯•é›† (X_valid)
        _, _, x_test, y_test = load_few_shot_data(test_wc, n_shots) # n_shotsè¿™é‡Œä¸å½±å“valid
        
        # æå–æµ‹è¯•é›†ç‰¹å¾
        print(f"    æå–æµ‹è¯•é›†ç‰¹å¾ ({len(x_test)} æ ·æœ¬)...")
        X_test_feat = np.array([extract_advanced_features(s) for s in tqdm(x_test, leave=False)])
        
        # æ‰§è¡Œæ£€ç´¢è¯„ä¼°
        acc = euclidean_retrieval(X_library, y_library, X_test_feat, y_test, NUM_NEIGHBORS)
        
        print(f"    ğŸ¯ WC{test_wc} è¯Šæ–­å‡†ç¡®ç‡: {acc:.2f}%")
        results.append(acc)

    # --- C. æ±‡æ€»æŠ¥å‘Š ---
    avg_acc = np.mean(results)
    print(f"\n{'='*60}")
    print(f"ğŸ† å®éªŒç»“æŸ | å¹³å‡å‡†ç¡®ç‡: {avg_acc:.2f}%")
    print(f"{'='*60}")
    
    return results, avg_acc

if __name__ == "__main__":
    # é…ç½®ï¼šæ‰€æœ‰çš„å·¥å†µéƒ½å‚ä¸çŸ¥è¯†åº“æ„å»ºï¼Œä¹Ÿå‚ä¸æµ‹è¯•
    # æ¨¡æ‹Ÿåœºæ™¯ï¼šæˆ‘ä»¬æœ‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰å·¥å†µæ•°æ®çš„åº“ï¼Œä½†æ¯ä¸ªå·¥å†µåªæœ‰æå°‘çš„æ ·æœ¬
    all_wcs = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    
    # è¿è¡Œå®éªŒ
    # è¿™é‡Œçš„ all_wcs æ—¢æ˜¯ source ä¹Ÿæ˜¯ targetï¼Œå› ä¸ºæˆ‘ä»¬è¦çœ‹ knowledge base èƒ½å¦è¦†ç›–æ‰€æœ‰æƒ…å†µ
    accuracies, avg = run_few_shot_experiment(train_wcs=all_wcs, test_wcs=all_wcs, n_shots=N_SHOTS)
    
    # ä¿å­˜ç»“æœåˆ°æ—¥å¿—
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "few_shot_test/result/log_fewshot"
    os.makedirs(log_dir, exist_ok=True)
    
    log_content = [
        f"Time: {timestamp}",
        f"Dataset: {DATASET}",
        f"N_Shots: {N_SHOTS}",
        f"Neighbors (k): {NUM_NEIGHBORS}",
        f"Train WCs: {all_wcs}",
        "-" * 30
    ]
    
    for wc, acc in zip(all_wcs, accuracies):
        log_content.append(f"Test WC{wc}: {acc:.2f}%")
    
    log_content.append("-" * 30)
    log_content.append(f"Average Accuracy: {avg:.2f}%")
    
    with open(os.path.join(log_dir, f"fewshot_results_{timestamp}.txt"), 'w') as f:
        f.write("\n".join(log_content))