import numpy as np
import os
import json
from tqdm import tqdm
from scipy import signal
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from scipy import stats  # å¢åŠ è¿™ä¸€è¡Œ

# =========================================================
# 1. ç®—æ³•éƒ¨åˆ† (å®Œå…¨å¤ç”¨ä½ æä¾›çš„ä»£ç )
# =========================================================

# def extract_advanced_features(time_series, fs=1000):
#     if time_series.shape[0] < time_series.shape[1]: 
#         time_series = time_series.T
#     n_channels = time_series.shape[1]
#     all_features = []
    
#     for ch in range(n_channels):
#         sig = time_series[:, ch]
#         # 0. åŸºç¡€å»å‡å€¼
#         sig = sig - np.mean(sig)
#         rms = np.sqrt(np.mean(sig**2)) + 1e-10
        
#         # 1. ã€æ ¸å¿ƒï¼šæ— é‡çº²æ—¶åŸŸç‰¹å¾ã€‘â€”â€” è¿™äº›æŒ‡æ ‡è·¨æœºå™¨éå¸¸é²æ£’
#         # å®ƒä»¬è¡¡é‡çš„æ˜¯â€œæœ‰å¤šåƒæ•…éšœâ€ï¼Œè€Œä¸æ˜¯â€œæŒ¯åŠ¨æœ‰å¤šå¤§â€
#         # kur = signal.kurtosis(sig)           # å³­åº¦ï¼ˆåæ˜ å†²å‡»ï¼‰
#         # skw = signal.skew(sig)
#         # # ååº¦ï¼ˆåæ˜ ä¸å¯¹ç§°ï¼‰
#         kur = stats.kurtosis(sig)           # å³­åº¦ï¼ˆåæ˜ å†²å‡»ï¼‰
#         skw = stats.skew(sig)               # ååº¦ï¼ˆåæ˜ ä¸å¯¹
#         crest = np.max(np.abs(sig)) / rms    # å³°å€¼å› å­
#         shape = rms / (np.mean(np.abs(sig)) + 1e-10) # æ³¢å½¢å› å­
#         impulse = np.max(np.abs(sig)) / (np.mean(np.abs(sig)) + 1e-10) # è„‰å†²å› å­
        
#         # 2. ã€æ ¸å¿ƒï¼šé¢‘è°±èƒ½é‡åˆ†å¸ƒã€‘â€”â€” å…³æ³¨èƒ½é‡åˆ†å¸ƒåœ¨å“ªäº›é¢‘æ®µ
#         f, Pxx = signal.welch(sig, fs=fs, nperseg=256)
#         # å°†é¢‘è°±å¹³åˆ†æˆ 8 ä¸ªé¢‘æ®µï¼Œè®¡ç®—æ¯ä¸ªé¢‘æ®µå æ€»èƒ½é‡çš„æ¯”ä¾‹
#         Pxx_norm = Pxx / (np.sum(Pxx) + 1e-10)
#         bands = np.array_split(Pxx_norm, 8)
#         band_energies = [np.sum(b) for b in bands]
        
#         all_features.extend([kur, skw, crest, shape, impulse] + band_energies)
        
#     return np.array(all_features)


def extract_advanced_features(time_series, fs=1000):
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
        
    n_channels = time_series.shape[1]
    all_features = []
    
    for ch in range(n_channels):
        signal_data = time_series[:, ch]
        
        # === [æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨æ­¤å¤„åŠ å…¥ä¿¡å·æ ‡å‡†åŒ–] ===
        # è¿™ä¸€è¡Œèƒ½æ¶ˆé™¤ä¸åŒæœºå™¨ä¼ æ„Ÿå™¨å¢ç›Šã€åŠŸç‡å¯¼è‡´çš„å¹…å€¼å·¨å¤§å·®å¼‚
        signal_data = (signal_data - np.mean(signal_data)) / (np.std(signal_data) + 1e-10)
        # =====================================

        # éšåçš„æ—¶åŸŸç‰¹å¾ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ç­‰ï¼‰å°†åŸºäºæ ‡å‡†åŒ–åçš„ä¿¡å·è®¡ç®—
        time_features = [
            np.mean(signal_data), # æ ‡å‡†åŒ–åè¯¥å€¼è¶‹äº0
            np.std(signal_data),  # æ ‡å‡†åŒ–åè¯¥å€¼è¶‹äº1
            np.max(np.abs(signal_data)),
            # ... å…¶ä½™ä»£ç ä¸å˜
            np.max(signal_data) - np.min(signal_data),
            np.sqrt(np.mean(signal_data**2)),
            np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
            np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
            np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
        ]
        
        # é¢‘åŸŸç‰¹å¾
        f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
        dominant_freq = f[np.argmax(Pxx)]
        spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10)
        
        freq_features = [
            dominant_freq,
            spectral_centroid,
            np.max(Pxx),
            np.mean(Pxx),
            np.std(Pxx)
        ]
        
        all_features.extend(time_features + freq_features)
    
    correlation_features = []
    for i in range(n_channels):
        for j in range(i+1, n_channels):
            corr = np.corrcoef(time_series[:, i], time_series[:, j])[0, 1]
            correlation_features.append(corr if not np.isnan(corr) else 0)
            
    return np.concatenate([np.array(all_features), np.array(correlation_features)])

def find_nearest_neighbors_weighted_feature(train_data, train_labels, test_data, num_neighbors):
    print("Extracting advanced features...")
    train_features = np.array([extract_advanced_features(seq) for seq in tqdm(train_data, desc="Train Feat")])
    test_features = np.array([extract_advanced_features(seq) for seq in tqdm(test_data, desc="Test Feat")])

    # --- [æ ¸å¿ƒä¿®æ”¹ 1ï¼šç‹¬ç«‹æ ‡å‡†åŒ–] ---
    # è®­ç»ƒé›†ç”¨è‡ªå·±çš„å‡å€¼æ–¹å·®ï¼Œæµ‹è¯•é›†ä¹Ÿç”¨è‡ªå·±çš„å‡å€¼æ–¹å·®
    # è¿™ä¸€æ­¥èƒ½å¼ºè¡ŒæŠµæ¶ˆæ‰ä¸åŒæœºå™¨å¸¦æ¥çš„å…¨å±€ç‰¹å¾åç§»
    train_features_scaled = StandardScaler().fit_transform(train_features)
    test_features_scaled = StandardScaler().fit_transform(test_features)
    
    print("Calculating supervised feature weights...")
    unique_classes = np.unique(train_labels)
    n_features = train_features_scaled.shape[1]
    feature_weights = np.zeros(n_features)
    
    for label in unique_classes:
        class_mask = (train_labels == label)
        class_data = train_features_scaled[class_mask]
        
        if len(class_data) > 1:
            class_var = np.var(class_data, axis=0)
            # --- [æ ¸å¿ƒä¿®æ”¹ 2ï¼šå¢å¤§å¹³æ»‘é¡¹] ---
            # è·¨æœºå™¨æ—¶ï¼Œæƒé‡ä¸èƒ½ç»™å¾—å¤ªæç«¯ï¼Œ0.1 æ˜¯å¹³è¡¡é²æ£’æ€§çš„ç»éªŒå€¼
            weight = 1.0 / (class_var + 0.2) 
            feature_weights += weight
    
    feature_weights = feature_weights / len(unique_classes)
    feature_weights = feature_weights / (np.max(feature_weights) + 1e-10)
    
    print("Applying feature weights...")
    train_weighted = train_features_scaled * feature_weights
    test_weighted = test_features_scaled * feature_weights
    
    print(f"Searching for {num_neighbors} nearest neighbors...")
    # --- [æ ¸å¿ƒä¿®æ”¹ 3ï¼šæ”¹ç”¨ä½™å¼¦è·ç¦»] ---
    # metric='cosine' åªçœ‹ç‰¹å¾å‘é‡çš„æ–¹å‘ï¼Œä¸çœ‹é•¿åº¦ï¼Œå¯¹è·¨æœºå™¨ç¯å¢ƒæå…¶æœ‰æ•ˆ
    # nbrs = NearestNeighbors(n_neighbors=num_neighbors, algorithm='auto', metric='cosine', n_jobs=-1)
    
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='cosine', n_jobs=-1)
    nbrs.fit(train_weighted)
    
    distances, indices = nbrs.kneighbors(test_weighted)

    results = []
    for test_index in range(len(test_data)):
        results.append({
            "test_index": test_index, 
            "neighbors": indices[test_index].tolist()
        })
    return results, train_weighted, test_weighted
    
    
def visualize_results(X_train_raw, X_test_raw, train_feat_w, test_feat_w, y_train, y_test, neighbor_results):
    import matplotlib.pyplot as plt
    from sklearn.manifold import TSNE
    
    # 1. ç©ºé—´ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE)
    print("æ­£åœ¨ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾ (t-SNE)...")
    tsne = TSNE(n_components=2, init='pca', random_state=42)
    all_feats = np.vstack([train_feat_w, test_feat_w])
    all_2d = tsne.fit_transform(all_feats)
    
    train_2d = all_2d[:len(train_feat_w)]
    test_2d = all_2d[len(train_feat_w):]

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    # ç»˜åˆ¶è®­ç»ƒé›† (ç”¨ç©ºå¿ƒåœ†è¡¨ç¤ºï¼Œé¢œè‰²åŒºåˆ†æ•…éšœ)
    for lbl in np.unique(y_train):
        idx = np.where(y_train == lbl)
        plt.scatter(train_2d[idx, 0], train_2d[idx, 1], label=f'Train-{lbl}', alpha=0.3, marker='o')
    # ç»˜åˆ¶æµ‹è¯•é›† (ç”¨æ˜Ÿå·è¡¨ç¤ºï¼Œé¢œè‰²åŒºåˆ†æ•…éšœ)
    for lbl in np.unique(y_test):
        idx = np.where(y_test == lbl)
        plt.scatter(test_2d[idx, 0], test_2d[idx, 1], label=f'Test-{lbl}', marker='x', edgecolors='black')
    plt.title("Spatial Feature Distribution (Weighted)")
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

    # 2. æŠ˜çº¿å›¾å¯¹æ¯” (å–æµ‹è¯•é›†ç¬¬1ä¸ªæ ·æœ¬åŠå…¶æœ€è¿‘é‚»)
    plt.subplot(1, 2, 2)
    test_idx = 0 
    nei_idx = neighbor_results[test_idx]['neighbors'][0] # æœ€è¿‘çš„ä¸€ä¸ª
    
    # è¿™é‡Œçš„ç»´åº¦æ˜¯ (1, 2048), å– [0] å˜æˆä¸€ç»´
    plt.plot(X_test_raw[test_idx][0], label=f'Test Sample (Class: {y_test[test_idx]})', alpha=0.8)
    plt.plot(X_train_raw[nei_idx][0], label=f'Nearest Neighbor (Class: {y_train[nei_idx]})', alpha=0.6, linestyle='--')
    plt.title("Raw Signal Comparison (Test vs Neighbor)")
    plt.legend()
    
    plt.tight_layout()
    plt.show() # åªæ˜¾ç¤ºï¼Œä¸ä¿å­˜

# =========================================================
# 2. æ£€ç´¢æ‰§è¡Œç¨‹åº
# =========================================================

def run_retrieval():
    ROOT = "cross_machine_test"
    DATA_ROOT = os.path.join(ROOT, "data")
    INDEX_ROOT = os.path.join(ROOT, "data_index")
    NUM_NEIGHBORS = 5
    
    # 1. å®šä¹‰æºï¼ˆè®­ç»ƒé›†/æ£€ç´¢åº“ï¼‰
    # å‡è®¾ä½¿ç”¨ BJTU WC1 ä½œä¸ºæº
    source_name = "BJTU_leftaxlebox/WC1"
    train_x_path = os.path.join(DATA_ROOT, source_name, "X_train.npy")
    train_y_path = os.path.join(DATA_ROOT, source_name, "y_train.npy")
    
    if not os.path.exists(train_x_path):
        print(f"é”™è¯¯: æ‰¾ä¸åˆ°æºæ•°æ® {train_x_path}")
        return

    X_train = np.load(train_x_path)
    y_train = np.load(train_y_path)
    
    # 2. å®šä¹‰æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ç›®æ ‡ï¼ˆæµ‹è¯•é›†ï¼‰
    # åŒ…æ‹¬ BJTU è‡ªå·±çš„éªŒè¯é›†å’Œ Ottawa çš„éªŒè¯é›†
    test_tasks = [
        {"name": "BJTU_leftaxlebox_WC1", "path": "BJTU_leftaxlebox/WC1"},
        {"name": "BJTU_gearbox_WC1", "path": "BJTU_gearbox/WC1"},
        # {"name": "Ottawa_A", "path": "Ottawa/A"},
        # {"name": "Ottawa_B", "path": "Ottawa/B"},
        # {"name": "Ottawa_C", "path": "Ottawa/C"},
        # {"name": "Ottawa_D", "path": "Ottawa/D"},
        # {"name": "swjtu",    "path": "swjtu/WC1"}
    ]
    
    for task in test_tasks:
        print(f"\nğŸš€ å¼€å§‹æ£€ç´¢ä»»åŠ¡: æŸ¥è¯¢ {task['name']} -> æº {source_name}")
        
        test_x_path = os.path.join(DATA_ROOT, task['path'], "X_valid.npy")
        if not os.path.exists(test_x_path):
            print(f"è·³è¿‡: æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ® {test_x_path}")
            continue
            
        X_test = np.load(test_x_path)
        
        # æ‰§è¡Œæ£€ç´¢ç®—æ³• (å–æœ€è¿‘çš„ 5 ä¸ªé‚»å±…)
        neighbor_results, train_feat_w, test_feat_w = find_nearest_neighbors_weighted_feature(
            train_data=X_train, train_labels=y_train, test_data=X_test, num_neighbors=NUM_NEIGHBORS
        )
        
        # === [æ–°å¢ï¼šè®¡ç®—èšç±»çº¯åº¦] ===
        test_y_path = os.path.join(DATA_ROOT, task['path'], "y_valid.npy")
        y_test = np.load(test_y_path)  # åŠ è½½æµ‹è¯•é›†çœŸå®æ ‡ç­¾
        
        total_correct = 0
        num_neighbors = len(neighbor_results[0]['neighbors'])
        
        for i, res in enumerate(neighbor_results):
            true_label = y_test[i]
            # è·å–è¿™ k ä¸ªé‚»å±…åœ¨è®­ç»ƒé›†é‡Œçš„æ ‡ç­¾
            neighbor_labels = y_train[res['neighbors']]
            # ç»Ÿè®¡å‘½ä¸­æ•°
            total_correct += np.sum(neighbor_labels == true_label)
            
        purity = (total_correct / (len(y_test) * num_neighbors)) * 100
        print(f"ğŸ“Š èšç±»çº¯åº¦ (Purity@k={num_neighbors}): {purity:.2f}%")
        # =========================
        
        # 3. [æ–°å¢] è°ƒç”¨å¯è§†åŒ–
        visualize_results(X_train, X_test, train_feat_w, test_feat_w, y_train, y_test, neighbor_results)
        
        
        # 3. ä¿å­˜ç»“æœ
        output_dir = os.path.join(INDEX_ROOT, task['name'])
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"nearest_{NUM_NEIGHBORS}_neighbors.json")
        
        with open(output_file, 'w') as f:
            json.dump(neighbor_results, f, indent=4)
            
        print(f"âœ… ä»»åŠ¡å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    run_retrieval()