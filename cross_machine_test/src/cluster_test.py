import numpy as np
import os
import json
from tqdm import tqdm
from scipy import signal
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import NearestNeighbors
from scipy import stats  # å¢åŠ è¿™ä¸€è¡Œ
import os
import numpy as np
import json
from dtaidistance import dtw_ndim
from tqdm import tqdm
from scipy.spatial.distance import cdist
from scipy.stats import skew, kurtosis
from scipy.fft import fft, fftfreq
import numpy as np
import os
from tqdm import tqdm
from scipy.spatial.distance import cdist
from scipy.stats import skew, kurtosis
from scipy.fft import fft
from sklearn.decomposition import PCA

# =========================================================
# 1. ç®—æ³•éƒ¨åˆ† (å®Œå…¨å¤ç”¨ä½ æä¾›çš„ä»£ç )
# =========================================================

# def extract_advanced_features(time_series, fs=1000):
#     if time_series.shape[0] < time_series.shape[1]: 
#         time_series = time_series.T
#     n_channels = time_series.shape[1]
#     all_features = []
    
#     for ch in range(n_channels):
#         sig = time_series[:, ch]
#         # 0. åŸºç¡€å»å‡å€¼
#         sig = sig - np.mean(sig)
#         rms = np.sqrt(np.mean(sig**2)) + 1e-10
        
#         # 1. ã€æ ¸å¿ƒï¼šæ— é‡çº²æ—¶åŸŸç‰¹å¾ã€‘â€”â€” è¿™äº›æŒ‡æ ‡è·¨æœºå™¨éå¸¸é²æ£’
#         # å®ƒä»¬è¡¡é‡çš„æ˜¯â€œæœ‰å¤šåƒæ•…éšœâ€ï¼Œè€Œä¸æ˜¯â€œæŒ¯åŠ¨æœ‰å¤šå¤§â€
#         # kur = signal.kurtosis(sig)           # å³­åº¦ï¼ˆåæ˜ å†²å‡»ï¼‰
#         # skw = signal.skew(sig)
#         # # ååº¦ï¼ˆåæ˜ ä¸å¯¹ç§°ï¼‰
#         kur = stats.kurtosis(sig)           # å³­åº¦ï¼ˆåæ˜ å†²å‡»ï¼‰
#         skw = stats.skew(sig)               # ååº¦ï¼ˆåæ˜ ä¸å¯¹
#         crest = np.max(np.abs(sig)) / rms    # å³°å€¼å› å­
#         shape = rms / (np.mean(np.abs(sig)) + 1e-10) # æ³¢å½¢å› å­
#         impulse = np.max(np.abs(sig)) / (np.mean(np.abs(sig)) + 1e-10) # è„‰å†²å› å­
        
#         # 2. ã€æ ¸å¿ƒï¼šé¢‘è°±èƒ½é‡åˆ†å¸ƒã€‘â€”â€” å…³æ³¨èƒ½é‡åˆ†å¸ƒåœ¨å“ªäº›é¢‘æ®µ
#         f, Pxx = signal.welch(sig, fs=fs, nperseg=256)
#         # å°†é¢‘è°±å¹³åˆ†æˆ 8 ä¸ªé¢‘æ®µï¼Œè®¡ç®—æ¯ä¸ªé¢‘æ®µå æ€»èƒ½é‡çš„æ¯”ä¾‹
#         Pxx_norm = Pxx / (np.sum(Pxx) + 1e-10)
#         bands = np.array_split(Pxx_norm, 8)
#         band_energies = [np.sum(b) for b in bands]
        
#         all_features.extend([kur, skw, crest, shape, impulse] + band_energies)
        
#     return np.array(all_features)


# def extract_advanced_features(time_series, fs=1000):
#     if time_series.shape[0] < time_series.shape[1]: 
#         time_series = time_series.T
        
#     n_channels = time_series.shape[1]
#     all_features = []
    
#     for ch in range(n_channels):
#         signal_data = time_series[:, ch]
        
#         # === [æ ¸å¿ƒä¿®æ”¹ï¼šåœ¨æ­¤å¤„åŠ å…¥ä¿¡å·æ ‡å‡†åŒ–] ===
#         # è¿™ä¸€è¡Œèƒ½æ¶ˆé™¤ä¸åŒæœºå™¨ä¼ æ„Ÿå™¨å¢ç›Šã€åŠŸç‡å¯¼è‡´çš„å¹…å€¼å·¨å¤§å·®å¼‚
#         # --- ä¿®æ”¹å‰ ---
# # signal_data = (signal_data - np.mean(signal_data)) / (np.std(signal_data) + 1e-10)

#         # --- ä¿®æ”¹å (ä¿ç•™ä¸€éƒ¨åˆ†å¹…å€¼å·®å¼‚ï¼ŒåŒæ—¶æ¶ˆé™¤å¢ç›Šåç§») ---
#         # ä½¿ç”¨å…¨å±€å¹³å‡æ ‡å‡†å·®çš„ç¼©æ”¾ï¼Œæˆ–è€…åªé™¤ä»¥æ ‡å‡†å·®çš„å¯¹æ•°
#         std_val = np.std(signal_data)
#         signal_data = (signal_data - np.mean(signal_data)) / (np.log1p(std_val) + 1.0)
#         # =====================================

#         # éšåçš„æ—¶åŸŸç‰¹å¾ï¼ˆå‡å€¼ã€æ ‡å‡†å·®ã€æœ€å¤§å€¼ç­‰ï¼‰å°†åŸºäºæ ‡å‡†åŒ–åçš„ä¿¡å·è®¡ç®—
#         time_features = [
#             np.mean(signal_data), # æ ‡å‡†åŒ–åè¯¥å€¼è¶‹äº0
#             np.std(signal_data),  # æ ‡å‡†åŒ–åè¯¥å€¼è¶‹äº1
#             np.max(np.abs(signal_data)),
#             # ... å…¶ä½™ä»£ç ä¸å˜
#             np.max(signal_data) - np.min(signal_data),
#             np.sqrt(np.mean(signal_data**2)),
#             np.max(np.abs(signal_data)) / (np.sqrt(np.mean(signal_data**2)) + 1e-10),
#             np.sum(signal_data**4) / (np.sum(signal_data**2)**2 + 1e-10),
#             np.sum((signal_data - np.mean(signal_data))**3) / (len(signal_data) * np.std(signal_data)**3 + 1e-10)
#         ]
        
#         # é¢‘åŸŸç‰¹å¾
#         f, Pxx = signal.welch(signal_data, fs=fs, nperseg=min(1024, len(signal_data)), noverlap=512)
#         dominant_freq = f[np.argmax(Pxx)]
#         spectral_centroid = np.sum(f * Pxx) / (np.sum(Pxx) + 1e-10)
        
#         freq_features = [
#             dominant_freq,
#             spectral_centroid,
#             np.max(Pxx),
#             np.mean(Pxx),
#             np.std(Pxx)
#         ]
        
#         all_features.extend(time_features + freq_features)
    
#     correlation_features = []
#     for i in range(n_channels):
#         for j in range(i+1, n_channels):
#             corr = np.corrcoef(time_series[:, i], time_series[:, j])[0, 1]
#             correlation_features.append(corr if not np.isnan(corr) else 0)

#         # --- åœ¨å‡½æ•° return ä¹‹å‰æ’å…¥ ---
#     # å¯¹å§‹ç»ˆä¸ºæ­£çš„ç»Ÿè®¡ç‰¹å¾è¿›è¡Œå¯¹æ•°å¤„ç†ï¼Œå‡å°‘ç¦»ç¾¤å€¼å½±å“
#     # å‡è®¾ features æ˜¯ä½ æœ€åæ‹¼æ¥å¥½çš„æ•°ç»„
#     # å¯ä»¥é’ˆå¯¹å‰å‡ ä¸ªæ—¶åŸŸç‰¹å¾åšå¤„ç†ï¼Œæ¯”å¦‚ï¼š
#     all_features = np.array(all_features)
#     # å¯¹ å³­åº¦ã€è„‰å†²å› å­ã€å³°å€¼å› å­ç­‰(é€šå¸¸æ˜¯å¤§äº0çš„)å–å¯¹æ•°
#     # å‡è®¾ç´¢å¼• 0, 2, 5, 6, 7 æ˜¯è¿™äº›ç‰¹å¾ï¼š
#     indices_to_log = [2, 5, 6, 7] 
#     all_features[indices_to_log] = np.log1p(np.abs(all_features[indices_to_log]))
            
#     return np.concatenate([np.array(all_features), np.array(correlation_features)])

def extract_advanced_features(time_series, fs=1000):
    """
    ä¿®æ”¹ç‰ˆï¼šç›´æ¥æå– FFT é¢‘è°±ç‰¹å¾ (å–å‰ 512 ä¸ªé¢‘ç‚¹)ã€‚
    é¢‘è°±å½¢çŠ¶å¯¹æ•…éšœç±»å‹æ›´æ•æ„Ÿï¼Œè€Œå¯¹å·¥å†µå¸¦æ¥çš„èƒ½é‡å˜åŒ–ç›¸å¯¹é²æ£’ã€‚
    """
    if time_series.shape[0] < time_series.shape[1]: 
        time_series = time_series.T
        
    n_channels = time_series.shape[1]
    all_features = []
    
    for ch in range(n_channels):
        sig = time_series[:, ch]
        
        # 1. ç®€å•çš„å»å‡å€¼
        sig = sig - np.mean(sig)
        
        # 2. è®¡ç®— FFT
        fft_vals = np.abs(fft(sig))
        
        # 3. åªå–å‰ä¸€åŠ (æ­£é¢‘ç‡éƒ¨åˆ†)ï¼Œé€šå¸¸å–å‰ 512 æˆ– 1024 ä¸ªç‚¹
        # å‡è®¾è¾“å…¥é•¿åº¦æ˜¯ 2048ï¼Œå–å‰ 512 ä¸ªç‚¹è¶³ä»¥æ¶µç›–ä¸»è¦æ•…éšœé¢‘æ®µ
        fft_half = fft_vals[:512] 
        
        # 4. [å…³é”®] å½’ä¸€åŒ–ï¼
        # é™¤ä»¥æœ€å¤§å€¼ï¼Œæ¶ˆé™¤è½¬é€Ÿå¸¦æ¥çš„ç»å¯¹èƒ½é‡å·®å¼‚ï¼Œåªä¿ç•™â€œå½¢çŠ¶â€
        fft_norm = fft_half / (np.max(fft_half) + 1e-10)
        
        all_features.extend(fft_norm)
            
    return np.array(all_features)

# def find_nearest_neighbors_weighted_feature(train_data, train_labels, test_data, num_neighbors):
#     print("Extracting features...")
#     train_features = np.array([extract_advanced_features(seq) for seq in tqdm(train_data, desc="Train Feat")])
#     test_features = np.array([extract_advanced_features(seq) for seq in tqdm(test_data, desc="Test Feat")])

#     # --- 1. ç»Ÿä¸€æ ‡å‡†åŒ–ï¼ˆå¿…é¡»ï¼‰ ---
#     scaler = StandardScaler()
#     train_features_scaled = scaler.fit_transform(train_features)
#     test_features_scaled = scaler.transform(test_features)
    
#     # --- 2. æ ¸å¿ƒæ”¹åŠ¨ï¼šæ”¾å¼ƒå¤æ‚çš„æƒé‡ï¼Œå›å½’æœ¬è´¨ ---
#     # åœ¨å¤šå·¥å†µä¸‹ï¼ŒFisher Score å¾€å¾€ä¼šå¤±æ•ˆã€‚æˆ‘ä»¬æ”¹ç”¨â€œæ–¹å·®å¹³æ»‘æƒé‡â€
#     # åªå‹ä½é‚£äº›å®Œå…¨æ˜¯å™ªå£°ï¼ˆæ–¹å·®æå¤§ä¸”æ— è§„å¾‹ï¼‰çš„ç‰¹å¾
#     feat_std = np.std(train_features_scaled, axis=0)
#     feature_weights = 1.0 / (feat_std + 0.5)  # ç®€å•çš„å€’æ•°å¹³æ»‘
#     feature_weights = feature_weights / np.max(feature_weights) 

#     # å¦‚æœä½ æ€€ç–‘æƒé‡è¿˜æ˜¯æœ‰é—®é¢˜ï¼Œå¯ä»¥ç›´æ¥å¼ºåˆ¶æ‰€æœ‰æƒé‡ä¸º 1ï¼š
#     # feature_weights = np.ones(train_features_scaled.shape[1]) 
    
#     train_weighted = train_features_scaled * feature_weights
#     test_weighted = test_features_scaled * feature_weights
    
#     # --- 3. æ ¸å¿ƒæ”¹åŠ¨ï¼šæ”¹å›æ¬§æ°è·ç¦» ---
#     # å½“ä½¿ç”¨ StandardScaler åï¼Œæ•°æ®ä¸­å¿ƒåœ¨ 0 ç‚¹ã€‚
#     # ä½™å¼¦è·ç¦»å¯¹ä¸­å¿ƒç‚¹é™„è¿‘çš„æ•°æ®æå…¶æ•æ„Ÿï¼Œä¼šå¯¼è‡´è¯†åˆ«æ··ä¹±ã€‚æ¬§æ°è·ç¦»åœ¨æ­¤æ—¶æ›´ç¨³å®šã€‚
#     print(f"Searching for {num_neighbors} neighbors using Euclidean distance...")
#     nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='euclidean', n_jobs=-1)
#     nbrs.fit(train_weighted)
    
#     distances, indices = nbrs.kneighbors(test_weighted)

#     results = []
#     for test_index in range(len(test_data)):
#         results.append({
#             "test_index": test_index, 
#             "neighbors": indices[test_index].tolist()
#         })
#     return results, train_weighted, test_weighted
    
    
def find_nearest_neighbors_weighted_feature(train_data, train_labels, test_data, num_neighbors):
    print("Extracting FFT features...")
    # ... (ç‰¹å¾æå–éƒ¨åˆ†ä»£ç ä¸ç”¨åŠ¨) ...
    train_features = np.array([extract_advanced_features(seq) for seq in tqdm(train_data, desc="Train Feat")])
    test_features = np.array([extract_advanced_features(seq) for seq in tqdm(test_data, desc="Test Feat")])

    # --- 1. ä»ç„¶ä¿ç•™ StandardScalerï¼Œæœ‰åŠ©äº Cosine è®¡ç®— ---
    scaler = StandardScaler()
    train_features_scaled = scaler.fit_transform(train_features)
    test_features_scaled = scaler.transform(test_features)
    
    # --- 2. æƒé‡å…¨ä¸º 1 (ä¸åŠ æƒ) ---
    print("Applying simplified weights (Ones)...")
    feature_weights = np.ones(train_features_scaled.shape[1])
    
    train_weighted = train_features_scaled * feature_weights
    test_weighted = test_features_scaled * feature_weights
    
    # --- 3. [å…³é”®ä¿®æ”¹] æ”¹ä¸º Cosine è·ç¦» ---
    print(f"Searching for {num_neighbors} nearest neighbors (Cosine)...")
    # metric='cosine' æ˜¯è·¨å·¥å†µ/è·¨å¹…å€¼å·®å¼‚çš„ç¥å™¨
    nbrs = NearestNeighbors(n_neighbors=num_neighbors, metric='cosine', n_jobs=-1)
    nbrs.fit(train_weighted)
    
    distances, indices = nbrs.kneighbors(test_weighted)

    results = []
    for test_index in range(len(test_data)):
        results.append({
            "test_index": test_index, 
            "neighbors": indices[test_index].tolist()
        })
    return results
    
# def visualize_results(X_train_raw, X_test_raw, train_feat_w, test_feat_w, y_train, y_test, neighbor_results):
# def visualize_results(X_train_raw, X_test_raw, y_train, y_test, neighbor_results):
#     import matplotlib.pyplot as plt
#     from sklearn.manifold import TSNE
    
#     # 1. ç©ºé—´ç‰¹å¾åˆ†å¸ƒå›¾ (t-SNE)
#     print("æ­£åœ¨ç”Ÿæˆç©ºé—´åˆ†å¸ƒå›¾ (t-SNE)...")
#     tsne = TSNE(n_components=2, init='pca', random_state=42)
#     all_feats = np.vstack([train_feat_w, test_feat_w])
#     all_2d = tsne.fit_transform(all_feats)
    
#     train_2d = all_2d[:len(train_feat_w)]
#     test_2d = all_2d[len(train_feat_w):]

#     plt.figure(figsize=(12, 5))
#     plt.subplot(1, 2, 1)
#     # ç»˜åˆ¶è®­ç»ƒé›† (ç”¨ç©ºå¿ƒåœ†è¡¨ç¤ºï¼Œé¢œè‰²åŒºåˆ†æ•…éšœ)
#     for lbl in np.unique(y_train):
#         idx = np.where(y_train == lbl)
#         plt.scatter(train_2d[idx, 0], train_2d[idx, 1], label=f'Train-{lbl}', alpha=0.3, marker='o')
#     # ç»˜åˆ¶æµ‹è¯•é›† (ç”¨æ˜Ÿå·è¡¨ç¤ºï¼Œé¢œè‰²åŒºåˆ†æ•…éšœ)
#     for lbl in np.unique(y_test):
#         idx = np.where(y_test == lbl)
#         plt.scatter(test_2d[idx, 0], test_2d[idx, 1], label=f'Test-{lbl}', marker='x', edgecolors='black')
#     plt.title("Spatial Feature Distribution (Weighted)")
#     plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')

#     # 2. æŠ˜çº¿å›¾å¯¹æ¯” (å–æµ‹è¯•é›†ç¬¬1ä¸ªæ ·æœ¬åŠå…¶æœ€è¿‘é‚»)
#     plt.subplot(1, 2, 2)
#     test_idx = 0 
#     nei_idx = neighbor_results[test_idx]['neighbors'][0] # æœ€è¿‘çš„ä¸€ä¸ª
    
#     # è¿™é‡Œçš„ç»´åº¦æ˜¯ (1, 2048), å– [0] å˜æˆä¸€ç»´
#     plt.plot(X_test_raw[test_idx][0], label=f'Test Sample (Class: {y_test[test_idx]})', alpha=0.8)
#     plt.plot(X_train_raw[nei_idx][0], label=f'Nearest Neighbor (Class: {y_train[nei_idx]})', alpha=0.6, linestyle='--')
#     plt.title("Raw Signal Comparison (Test vs Neighbor)")
#     plt.legend()
    
#     plt.tight_layout()
#     plt.show() # åªæ˜¾ç¤ºï¼Œä¸ä¿å­˜

# =========================================================
# 2. æ£€ç´¢æ‰§è¡Œç¨‹åº
# =========================================================

def run_retrieval():
    ROOT = "cross_machine_test"
    DATA_ROOT = os.path.join(ROOT, "data")
    INDEX_ROOT = os.path.join(ROOT, "data_index")
    NUM_NEIGHBORS = 5
    
    # --- ä¿®æ”¹éƒ¨åˆ†ï¼šå¾ªç¯åŠ è½½æ‰€æœ‰ WC å·¥å†µå¹¶åˆå¹¶ ---
    source_category = "BJTU_leftaxlebox"
    all_X_train = []
    all_y_train = []
    
    print(f"æ­£åœ¨åˆå¹¶ {source_category} çš„æ‰€æœ‰æºåŸŸå·¥å†µ (WC1-WC9)...")
    for i in range(1, 10):
        wc_path = os.path.join(DATA_ROOT, source_category, f"WC{i}")
        tx_p = os.path.join(wc_path, "X_train.npy")
        ty_p = os.path.join(wc_path, "y_train.npy")
        
        if os.path.exists(tx_p):
            all_X_train.append(np.load(tx_p))
            all_y_train.append(np.load(ty_p))
    
    if not all_X_train:
        print("é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•æºåŸŸæ•°æ®")
        return

    # åˆå¹¶æ•°ç»„
    X_train = np.vstack(all_X_train)
    y_train = np.concatenate(all_y_train)
    source_name = f"{source_category}_ALL_WC" # æ›´æ–°æ ‡è¯†å
    
    # 2. å®šä¹‰æ‰€æœ‰éœ€è¦æŸ¥è¯¢çš„ç›®æ ‡ï¼ˆæµ‹è¯•é›†ï¼‰
    # åŒ…æ‹¬ BJTU è‡ªå·±çš„éªŒè¯é›†å’Œ Ottawa çš„éªŒè¯é›†
    test_tasks = [
        {"name": "BJTU_leftaxlebox_WC1", "path": "BJTU_leftaxlebox/WC1"},
        {"name": "BJTU_gearbox_WC1", "path": "BJTU_gearbox/WC1"},
        # {"name": "Ottawa_A", "path": "Ottawa/A"},
        # {"name": "Ottawa_B", "path": "Ottawa/B"},
        # {"name": "Ottawa_C", "path": "Ottawa/C"},
        # {"name": "Ottawa_D", "path": "Ottawa/D"},
        # {"name": "swjtu",    "path": "swjtu/WC1"}
    ]
    
    for task in test_tasks:
        print(f"\nğŸš€ å¼€å§‹æ£€ç´¢ä»»åŠ¡: æŸ¥è¯¢ {task['name']} -> æº {source_name}")
        
        test_x_path = os.path.join(DATA_ROOT, task['path'], "X_valid.npy")
        if not os.path.exists(test_x_path):
            print(f"è·³è¿‡: æ‰¾ä¸åˆ°æµ‹è¯•æ•°æ® {test_x_path}")
            continue
            
        X_test = np.load(test_x_path)
        
        # æ‰§è¡Œæ£€ç´¢ç®—æ³• (å–æœ€è¿‘çš„ 5 ä¸ªé‚»å±…)
        # neighbor_results, train_feat_w, test_feat_w = find_nearest_neighbors_weighted_feature(
        #     train_data=X_train, train_labels=y_train, test_data=X_test, num_neighbors=NUM_NEIGHBORS
        # )
        
        neighbor_results = find_nearest_neighbors_weighted_feature(
            train_data=X_train, train_labels=y_train, test_data=X_test, num_neighbors=NUM_NEIGHBORS
        )
        
        # === [æ–°å¢ï¼šè®¡ç®—èšç±»çº¯åº¦] ===
        test_y_path = os.path.join(DATA_ROOT, task['path'], "y_valid.npy")
        y_test = np.load(test_y_path)  # åŠ è½½æµ‹è¯•é›†çœŸå®æ ‡ç­¾
        
        total_correct = 0
        num_neighbors = len(neighbor_results[0]['neighbors'])
        
        for i, res in enumerate(neighbor_results):
            true_label = y_test[i]
            # è·å–è¿™ k ä¸ªé‚»å±…åœ¨è®­ç»ƒé›†é‡Œçš„æ ‡ç­¾
            neighbor_labels = y_train[res['neighbors']]
            # ç»Ÿè®¡å‘½ä¸­æ•°
            total_correct += np.sum(neighbor_labels == true_label)
            
        purity = (total_correct / (len(y_test) * num_neighbors)) * 100
        print(f"ğŸ“Š èšç±»çº¯åº¦ (Purity@k={num_neighbors}): {purity:.2f}%")
        # =========================
        
        # 3. [æ–°å¢] è°ƒç”¨å¯è§†åŒ–
        # visualize_results(X_train, X_test, train_feat_w, test_feat_w, y_train, y_test, neighbor_results)
        # visualize_results(X_train, X_test,  y_train, y_test, neighbor_results)
        
        # 3. ä¿å­˜ç»“æœ
        output_dir = os.path.join(INDEX_ROOT, task['name'])
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"nearest_{NUM_NEIGHBORS}_neighbors.json")
        
        with open(output_file, 'w') as f:
            json.dump(neighbor_results, f, indent=4)
            
        print(f"âœ… ä»»åŠ¡å®Œæˆ! ç»“æœå·²ä¿å­˜è‡³: {output_file}")

if __name__ == "__main__":
    run_retrieval()