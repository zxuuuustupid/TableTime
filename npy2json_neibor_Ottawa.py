from src.dataset_index import generate_json
from src.neighbor_find import *

def load_labels_as_map(label_file_path):
    """
    åŠ è½½æ ‡ç­¾ JSON æ–‡ä»¶ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºä¸€ä¸ªæ˜“äºæŸ¥æ‰¾çš„å­—å…¸ï¼ˆæ˜ å°„ï¼‰ã€‚
    
    Args:
        label_file_path (str): æ ‡ç­¾æ–‡ä»¶çš„è·¯å¾„ã€‚

    Returns:
        dict: ä¸€ä¸ªä» index (int) åˆ° label (str) çš„æ˜ å°„, e.g., {0: 'G0', 1: 'G0', ...}
              å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨æˆ–æ ¼å¼é”™è¯¯ï¼Œè¿”å› Noneã€‚
    """
    try:
        with open(label_file_path, 'r', encoding='utf-8') as f:
            label_list = json.load(f)
        
        # å°† [{"index": 0, "label": "G0"}, ...] è½¬æ¢ä¸º {0: "G0", ...}
        label_map = {item['index']: item['label'] for item in label_list}
        return label_map
    except FileNotFoundError:
        print(f"[ERROR] æ ‡ç­¾æ–‡ä»¶æœªæ‰¾åˆ°: {label_file_path}")
    except (json.JSONDecodeError, KeyError) as e:
        print(f"[ERROR] æ ‡ç­¾æ–‡ä»¶æ ¼å¼é”™è¯¯. éœ€è¦ 'index' å’Œ 'label' é”®. é”™è¯¯: {e}")
    return None


def calculate_retrieval_accuracy(retrieval_results_path, test_labels_path, train_labels_path):
    """
    è®¡ç®—æœ€è¿‘é‚»æ£€ç´¢çš„å‡†ç¡®åº¦ï¼ˆçº¯åº¦ï¼‰ã€‚

    Args:
        retrieval_results_path (str): æ£€ç´¢ç»“æœçš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
        test_labels_path (str): æµ‹è¯•é›†çœŸå®æ ‡ç­¾çš„ JSON æ–‡ä»¶è·¯å¾„ã€‚
        train_labels_path (str): è®­ç»ƒé›†çœŸå®æ ‡ç­¾çš„ JSON æ–‡ä»¶è·¯å¾„ (ç”¨äºæŸ¥æ‰¾é‚»å±…çš„æ ‡ç­¾)ã€‚
    """
    
    # 1. åŠ è½½æ‰€æœ‰å¿…è¦çš„æ ‡ç­¾æ•°æ®
    test_label_map = load_labels_as_map(test_labels_path)
    train_label_map = load_labels_as_map(train_labels_path)
    
    try:
        with open(retrieval_results_path, 'r', encoding='utf-8') as f:
            retrieval_data = json.load(f)
    except FileNotFoundError:
        print(f"[ERROR] æ£€ç´¢ç»“æœæ–‡ä»¶æœªæ‰¾åˆ°: {retrieval_results_path}")
        return
    except json.JSONDecodeError as e:
        print(f"[ERROR] æ£€ç´¢ç»“æœæ–‡ä»¶æ ¼å¼é”™è¯¯. é”™è¯¯: {e}")
        return

    if not test_label_map or not train_label_map:
        print("æ— æ³•ç»§ç»­è¯„ä¼°ï¼Œå› ä¸ºæ ‡ç­¾æ–‡ä»¶åŠ è½½å¤±è´¥ã€‚")
        return

    all_purities = []
    
    # 2. éå†æ¯ä¸€ä¸ªæµ‹è¯•æ ·æœ¬çš„æ£€ç´¢ç»“æœ
    for result_item in retrieval_data:
        test_index = result_item.get('test_index')
        neighbor_indices = result_item.get('neighbors', [])
        
        if test_index is None or not neighbor_indices:
            print(f"[WARNING] è·³è¿‡ test_index {test_index}ï¼Œå› ä¸ºæ•°æ®ä¸å®Œæ•´ã€‚")
            continue
            
        # è·å–å½“å‰æµ‹è¯•æ ·æœ¬çš„çœŸå®æ ‡ç­¾
        true_test_label = test_label_map.get(test_index)
        if true_test_label is None:
            print(f"[WARNING] åœ¨æ ‡ç­¾æ–‡ä»¶ä¸­æ‰¾ä¸åˆ° test_index {test_index} çš„çœŸå®æ ‡ç­¾ã€‚")
            continue
            
        # 3. è®¡ç®—é‚»å±…çº¯åº¦
        correct_neighbors = 0
        for neighbor_idx in neighbor_indices:
            # ä»è®­ç»ƒé›†æ ‡ç­¾æ˜ å°„ä¸­æŸ¥æ‰¾é‚»å±…çš„æ ‡ç­¾
            neighbor_label = train_label_map.get(neighbor_idx)
            
            if neighbor_label is not None and neighbor_label == true_test_label:
                correct_neighbors += 1
        
        # çº¯åº¦ = (ä¸æµ‹è¯•æ ·æœ¬åŒç±»çš„é‚»å±…æ•°) / (æ€»é‚»å±…æ•°)
        purity = correct_neighbors / len(neighbor_indices)
        all_purities.append(purity)
        
    # 4. è®¡ç®—å¹¶æ‰“å°æ€»ä½“ç»“æœ
    if not all_purities:
        print("[ERROR] æ²¡æœ‰å¯ä¾›è¯„ä¼°çš„æœ‰æ•ˆæ£€ç´¢ç»“æœã€‚")
        return 0.0
    
    mean_accuracy = np.mean(all_purities) * 100
    
    print("\n" + "="*50)
    print(f"[INFO] æœ€è¿‘é‚»æ£€ç´¢ç²¾åº¦è¯„ä¼°æŠ¥å‘Š")
    print(f"[INFO] - æ£€ç´¢æ–‡ä»¶: {os.path.basename(retrieval_results_path)}")
    print("="*50)
    print(f"[INFO] æ€»è®¡è¯„ä¼°çš„æµ‹è¯•æ ·æœ¬æ•°: {len(all_purities)}")
    print(f"[INFO] å¹³å‡æ£€ç´¢ç²¾åº¦ (Mean Purity @ k): {mean_accuracy:.2f}%")
    print("="*50)
    print(f"[INFO] (è¯¥æŒ‡æ ‡è¡¡é‡çš„æ˜¯ï¼šå¯¹äºä¸€ä¸ªæµ‹è¯•æ ·æœ¬ï¼Œå…¶æ‰¾åˆ°çš„é‚»å±…æœ‰å¤šå¤§æ¦‚ç‡ä¸å®ƒè‡ªå·±æ˜¯åŒä¸€ç±»åˆ«)")
    
    return mean_accuracy


# weight_DTW=0.1
# weight_feature=0.9
# def pipeline():
#     generate_json(dataset=dataset)
#     neighbor_find(dataset=dataset,
#                     train_work_condition_num=train_work_condition_num,
#                     test_work_condition_num=test_work_condition_num,
#                     dist_map = dist_map,
#                     neighbor_num = neighbor_num,
#                     skip_labels = None,)
#     calculate_retrieval_accuracy(retrieval_results_path=os.path.join("data_index", dataset, f"test_WC{test_work_condition_num}_train_WC{train_work_condition_num}",f"{list(dist_map.keys())[0]}_dist", f'nearest_{neighbor_num}_neighbors.json'),test_labels_path=os.path.join("data", "index",dataset,f"WC{test_work_condition_num}","test_index.json"), train_labels_path=os.path.join("data", "index", dataset,f"WC{train_work_condition_num}", "train_index.json"))

def pipeline(dataset, train_nums, test_num, dist_map, neighbor_num):
    # 1. ç”ŸæˆåŸºç¡€ç´¢å¼• (å¦‚æœéœ€è¦)
    generate_json(dataset=dataset)
    
    # 2. å¯»æ‰¾è¿‘é‚» (æ³¨æ„è¿™é‡Œä¼ çš„æ˜¯åˆ—è¡¨ train_nums)
    neighbor_find(dataset=dataset,
                  train_work_condition_nums=train_nums,
                  test_work_condition_num=test_num,
                  dist_map=dist_map,
                  neighbor_num=neighbor_num)
    
    # 3. æ„é€ è·¯å¾„æ ‡è¯† (ä¾‹å¦‚ [1,2,3] -> "1_2_3")
    train_tag = "_".join(map(str, train_nums))
    results_path = os.path.join("data_index", dataset, f"test_WC{test_num}_train_WCs{train_tag}", 
                                f"{list(dist_map.keys())[0]}_dist", f'nearest_{neighbor_num}_neighbors.json')
    
    # 4. åˆå¹¶è®­ç»ƒé›†æ ‡ç­¾ (æ ¸å¿ƒæ”¹åŠ¨ï¼šå› ä¸ºåˆå¹¶åçš„è®­ç»ƒé›†ç´¢å¼•æ˜¯è¿ç»­çš„ï¼Œéœ€è¦æ‰‹åŠ¨åˆå¹¶å­—å…¸)
    merged_train_labels = {}
    current_offset = 0
    for wc in train_nums:
        path = os.path.join("data", "index", dataset, f"WC{wc}", "train_index.json")
        with open(path, 'r') as f:
            labels = json.load(f)
            for item in labels:
                # å°†è¯¥å·¥å†µçš„æ ‡ç­¾å­˜å…¥åˆå¹¶å­—å…¸ï¼Œé”®ä¸ºå…¨å±€åç§»åçš„ç´¢å¼•
                merged_train_labels[current_offset] = item['label']
                current_offset += 1
    
    # 5. åŠ è½½æµ‹è¯•é›†æ ‡ç­¾
    test_labels_path = os.path.join("data", "index", dataset, f"WC{test_num}", "test_index.json")
    
    # 6. è®¡ç®—å‡†ç¡®ç‡ (è¿™é‡Œéœ€è¦ç¨å¾®ä¿®æ”¹ calculate_retrieval_accuracy ä½¿å…¶æ”¯æŒç›´æ¥ä¼ å­—å…¸ï¼Œæˆ–è€…å¦‚ä¸‹å¿«æ·å¤„ç†)
    # ä¸ºäº†æœ€å°åŒ–æ”¹åŠ¨ï¼Œæˆ‘ä»¬ä¸´æ—¶å†™ä¸€ä¸ªåˆå¹¶åçš„json
    temp_train_labels_path = "temp_merged_train_labels.json"
    with open(temp_train_labels_path, 'w') as f:
        json.dump([{"index": k, "label": v} for k, v in merged_train_labels.items()], f)
        
    acc=calculate_retrieval_accuracy(retrieval_results_path=results_path,
                                 test_labels_path=test_labels_path,
                                 train_labels_path=temp_train_labels_path)

    return acc
    
# if __name__ == "__main__":
        
#     dataset='BJTU-gearbox'
#     dist_map = {'FIW': find_nearest_neighbors_weighted_feature}
#     neighbor_num = 15
#     train_work_condition_num=1
#     test_work_condition_num=2
# #     pipeline()

# if __name__ == "__main__":
#     import datetime # ç¡®ä¿å¯¼å…¥ datetime
    
#     # dataset = 'BJTU-gearbox'
#     # dataset = 'BJTU-motor'
#     dataset = 'BJTU-leftaxlebox'
#     dist_map_name = 'FIW'
#     dist_map = {dist_map_name: find_nearest_neighbors_weighted_feature}
#     neighbor_num = 15
#     all_wcs = [1, 2, 3, 4, 5, 6, 7, 8, 9]

#     all_train_scenarios = [
#         [1],
#         [1,2],
#         [1,2,3],
#         [1,2,3,4],
#         [1,2,3,4,5],
#         [1,2,3,4,5,6,],
#         [1,2,3,4,5,6,7],
#         [1,2,3,4,5,6,7,8],
#         # [1,2,3,4,5,7,8]
#     ]
    
#     # ç”¨äºæ”¶é›†æ‰€æœ‰å®éªŒç»“æœçš„åˆ—è¡¨
#     experiment_logs = []
    
#     # --- å¼€å§‹å¤§å¾ªç¯ ---
#     for train_nums in all_train_scenarios:
#         test_wcs = [wc for wc in all_wcs if wc not in train_nums]
#         # test_wcs = [3,4,5,6,7,8,9]
        
#         print(f"\n{'='*60}")
#         print(f"ğŸš€ å¤§å®éªŒå¯åŠ¨ï¼šè®­ç»ƒé›†ç»„åˆ = {train_nums}")
#         print(f"{'='*60}")
        
#         scenario_accuracies = []
        
#         for test_wc in test_wcs:
#             print(f"\n>>> [å½“å‰é…ç½®] è®­ç»ƒ: {train_nums} | æµ‹è¯•: WC{test_wc}")
#             # è·å–å‡†ç¡®ç‡
#             acc = pipeline(dataset, train_nums, test_wc, dist_map, neighbor_num)
            
#             # è®°å½•å•æ¬¡ç»“æœ
#             log_str = f"Train: {train_nums} | Test: WC{test_wc} | Accuracy: {acc:.2f}%"
#             experiment_logs.append(log_str)
#             scenario_accuracies.append(acc)
        
#         # è®°å½•è¯¥åœºæ™¯çš„å¹³å‡å‡†ç¡®ç‡
#         avg_acc = np.mean(scenario_accuracies) if scenario_accuracies else 0
#         experiment_logs.append(f"--- Scenario Average (Train {train_nums}): {avg_acc:.2f}% ---\n")

#     # --- å®éªŒç»“æŸï¼Œä¿å­˜æ±‡æ€»ç»“æœ ---
    
#     # 1. ç”Ÿæˆæ–‡ä»¶å
#     timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
#     log_dir = "result/log"
#     os.makedirs(log_dir, exist_ok=True)
#     filename = f"{dataset}_{dist_map_name}_{timestamp}.txt"
#     filepath = os.path.join(log_dir, filename)
    
#     # 2. æ„å»ºå®Œæ•´æŠ¥å‘Šå†…å®¹
#     final_report = []
#     final_report.append("="*60)
#     final_report.append(f"å®éªŒæ±‡æ€»æŠ¥å‘Š")
#     final_report.append(f"æ—¶é—´: {timestamp}")
#     final_report.append(f"æ•°æ®é›†: {dataset}")
#     final_report.append(f"è·ç¦»åº¦é‡: {dist_map_name}")
#     final_report.append(f"é‚»å±…æ•°: {neighbor_num}")
#     final_report.append("="*60 + "\n")
#     final_report.extend(experiment_logs)
    
#     final_report_str = "\n".join(final_report)
    
#     # 3. æ‰“å°å¹¶ä¿å­˜
#     print("\n" + "#"*60)
#     print("å®éªŒå…¨éƒ¨å®Œæˆï¼æ±‡æ€»ç»“æœå¦‚ä¸‹ï¼š")
#     print("#"*60)
#     print(final_report_str)
    
#     with open(filepath, 'w', encoding='utf-8') as f:
#         f.write(final_report_str)
        
#     print(f"\n[INFO] æ±‡æ€»æ—¥å¿—å·²ä¿å­˜è‡³: {filepath}")


if __name__ == "__main__":
    import datetime 
    
    # 1. ä¿®æ”¹æ•°æ®é›†åç§° (å¿…é¡»ä¸ DataGenerator ç”Ÿæˆçš„æ–‡ä»¶å¤¹åä¸€è‡´)
    dataset = 'Ottawa' 
    
    dist_map_name = 'FIW'
    dist_map = {dist_map_name: find_nearest_neighbors_weighted_feature}
    neighbor_num = 15
    
    # 2. ä¿®æ”¹æ‰€æœ‰å·¥å†µåˆ—è¡¨ (Ottawa åªæœ‰ A,B,C,D -> WC1, WC2, WC3, WC4)
    all_wcs = [1, 2, 3, 4]
    
    # 3. ä¿®æ”¹è®­ç»ƒåœºæ™¯ç»„åˆ (æ³¨æ„æ•°å­—ä¸èƒ½è¶…è¿‡ 4)
    all_train_scenarios = [
        [1],          # å•å·¥å†µè®­ç»ƒ (ç”¨Aæµ‹B,C,D)
        [1, 2],       # åŒå·¥å†µè®­ç»ƒ (ç”¨A,Bæµ‹C,D)
        [1, 3, 2],    # ä¸‰å·¥å†µè®­ç»ƒ (ç”¨A,B,Cæµ‹D)
        # ä¹Ÿå¯ä»¥åšåå‘æ³›åŒ–ï¼Œä¾‹å¦‚ç”¨ D æµ‹ A
        # [4] 
    ]
    
    # ç”¨äºæ”¶é›†æ‰€æœ‰å®éªŒç»“æœçš„åˆ—è¡¨
    experiment_logs = []
    
    # --- å¼€å§‹å¤§å¾ªç¯ ---
    for train_nums in all_train_scenarios:
        # è‡ªåŠ¨è®¡ç®—æµ‹è¯•é›†ï¼šåœ¨ all_wcs é‡Œï¼Œä½†ä¸åœ¨è®­ç»ƒé›†é‡Œçš„
        test_wcs = [wc for wc in all_wcs if wc not in train_nums]
        
        print(f"\n{'='*60}")
        print(f"ğŸš€ å¤§å®éªŒå¯åŠ¨ï¼šè®­ç»ƒé›†ç»„åˆ = {train_nums}")
        print(f"{'='*60}")
        
        scenario_accuracies = []
        
        for test_wc in test_wcs:
            print(f"\n>>> [å½“å‰é…ç½®] è®­ç»ƒ: {train_nums} | æµ‹è¯•: WC{test_wc}")
            # è·å–å‡†ç¡®ç‡
            acc = pipeline(dataset, train_nums, test_wc, dist_map, neighbor_num)
            
            # è®°å½•å•æ¬¡ç»“æœ
            log_str = f"Train: {train_nums} | Test: WC{test_wc} | Accuracy: {acc:.2f}%"
            experiment_logs.append(log_str)
            scenario_accuracies.append(acc)
        
        # è®°å½•è¯¥åœºæ™¯çš„å¹³å‡å‡†ç¡®ç‡
        avg_acc = np.mean(scenario_accuracies) if scenario_accuracies else 0
        experiment_logs.append(f"--- Scenario Average (Train {train_nums}): {avg_acc:.2f}% ---\n")

    # --- å®éªŒç»“æŸï¼Œä¿å­˜æ±‡æ€»ç»“æœ (ä»£ç ä¿æŒä¸å˜) ---
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    log_dir = "result/log"
    os.makedirs(log_dir, exist_ok=True)
    filename = f"{dataset}_{dist_map_name}_{timestamp}.txt"
    filepath = os.path.join(log_dir, filename)
    
    final_report = []
    final_report.append("="*60)
    final_report.append(f"å®éªŒæ±‡æ€»æŠ¥å‘Š")
    final_report.append(f"æ—¶é—´: {timestamp}")
    final_report.append(f"æ•°æ®é›†: {dataset}")
    final_report.append(f"è·ç¦»åº¦é‡: {dist_map_name}")
    final_report.append(f"é‚»å±…æ•°: {neighbor_num}")
    final_report.append("="*60 + "\n")
    final_report.extend(experiment_logs)
    
    final_report_str = "\n".join(final_report)
    
    print("\n" + "#"*60)
    print("å®éªŒå…¨éƒ¨å®Œæˆï¼æ±‡æ€»ç»“æœå¦‚ä¸‹ï¼š")
    print("#"*60)
    print(final_report_str)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(final_report_str)
        
    print(f"\n[INFO] æ±‡æ€»æ—¥å¿—å·²ä¿å­˜è‡³: {filepath}")